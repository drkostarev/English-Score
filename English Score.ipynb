{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66fd38e",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#План-работы\" data-toc-modified-id=\"План-работы-1\">План работы</a></span></li><li><span><a href=\"#Загрузка-данных\" data-toc-modified-id=\"Загрузка-данных-2\">Загрузка данных</a></span></li><li><span><a href=\"#Очистка-данных\" data-toc-modified-id=\"Очистка-данных-3\">Очистка данных</a></span></li><li><span><a href=\"#Создание-модели\" data-toc-modified-id=\"Создание-модели-4\">Создание модели</a></span></li><li><span><a href=\"#Общий-вывод\" data-toc-modified-id=\"Общий-вывод-5\">Общий вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c5f84",
   "metadata": {},
   "source": [
    "# Определение уровня английского языка по субтитрам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8bdeef",
   "metadata": {},
   "source": [
    "Просмотр фильмов на языке оригинала – популярный и эффективный способ изучения иностранного языка. Для того, чтобы сделать этот процесс эффективным, необходимо выбрать фильм, который подходит ученику по уровню, т.е. такой фильм, в котором понятно 50-70% диалогов. Если в фильме понятна меньшая часть диалогов, то просмотр фильма сильно затрудняется и становится неинтересным, а если большая, то не происходит обучения. Для того, чтобы оценить уровень английского языка преподавателю необходимо посмотреть фильм целиком, что требует много времени. В рамках данной работы будет предпринята попытка создать модель, предсказывающую уровень языка на основе субтитров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07c4e6",
   "metadata": {},
   "source": [
    "## План работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f41b1e",
   "metadata": {},
   "source": [
    "На входе у нас есть набор субтитров разных фильмов с оцененным уровнем английского языка. Основная часть работы будет посвящена подготовке этих данных для моделирования: очистке и лемматизации.\n",
    "<br>В работе будет применен алгоритм частотно-инверсной частоты документа (tf-idf) для снижения веса часто встречающихся слов.\n",
    "<br>В конце необходимо будет выполнить задачу многоклассовой классификации на основе модели логистической регрессии.\n",
    "\n",
    "Этапы работы следующие:\n",
    "1. Загрузить данные\n",
    "2. Произвести очистку данных\n",
    "3. Подготовить выборки\n",
    "4. Собрать пайплайн для построения модели логистической регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c26b85",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3ec85",
   "metadata": {},
   "source": [
    "Сторонние библиотеки, которые были установлены для выполнения работы (закомментированы, потому что нет необходимости устанавливать их каждый раз):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101a005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pysrt\n",
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269ee93",
   "metadata": {},
   "source": [
    "Импортируем библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bde321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import spacy\n",
    "import pysrt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76eb2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки SpaCy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Создание словаря стоп-слов\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Настройки токенизаторов:\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c5292",
   "metadata": {},
   "source": [
    "Создадим словарь уровней английского языка для удобства:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9aefc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = {'A1': 1, \n",
    "          'A2': 2,\n",
    "          'B1': 3,\n",
    "          'B2': 4,\n",
    "          'C1': 5,\n",
    "          'C2': 6,\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b8742",
   "metadata": {},
   "source": [
    "Попробуем открыть один из файлов с субтитрами и проанализировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1c6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<font color=\"#ffff80\"><b>Fixed & Synced by bozxphd. Enjoy The Flick</b></font>\n",
      "(CLANGING)\n",
      "(DRAWER CLOSES)\n",
      "(INAUDIBLE)\n",
      "(CELL PHONE RINGING)\n",
      "BEN ON PHONE: <i>Michelle,<br/>please don't hang up.</i>\n",
      "<i>Just talk to me, okay?<br/>I can't believe you just left.</i>\n",
      "<i>Michelle.</i>\n",
      "<i>Come back.</i>\n",
      "<i>Please say something.</i>\n",
      "<i>Michelle, talk to me.</i>\n",
      "<i>Look, we had an argument.<br/>Couples fight.</i>\n",
      "<i>That is no reason<br/>to just leave everything behind.</i>\n",
      "<i>Running away isn't gonna help it any.<br/>Michelle, please...</i>\n",
      "(DIALTONE)\n",
      "NEWSCASTER: More details on that.\n",
      "<i>Elsewhere today,<br/>power has still not been restored</i>\n",
      "<i>to many cities on the southern seaboard</i>\n",
      "<i>in the wake of<br/>this afternoon's widespread blackout.</i>\n",
      "<i>While there had been<br/>some inclement weather in the region,</i>\n",
      "<i>the problem seems linked to<br/>what authorities are calling</i>\n",
      "<i>a catastrophic power surge<br/>that has crippled traffic in the area.</i>\n",
      "- (LOUD CRASH)<br/>- (GRUNTS)\n",
      "- (TIRES SCREECHING)<br/>- (SCREAMS)\n",
      "- (GLASS SHATTERING)<br/>- (GASPING)\n",
      "(GROANS)\n",
      "(HORN HONKING)\n",
      "(INHALES DEEPLY)\n",
      "(SNIFFS)\n",
      "(SIGHS)\n",
      "(GASPING)\n",
      "(CHAINS RATTLING)\n",
      "(BREATHING HEAVILY)\n",
      "(GRUNTING)\n",
      "(GROANING)\n",
      "(GROANING)\n",
      "(GRUNTING)\n",
      "(SOBS) No.\n",
      "(CHAIN JANGLING)\n",
      "No! (SOBBING)\n",
      "(BREATHING HEAVILY)\n",
      "Damn.\n",
      "(CLATTERS)\n",
      "(GRUNTS)\n",
      "(RUMBLING)\n",
      "(FOOTSTEPS APPROACHING)\n",
      "(PANTING)\n",
      "(GASPS)\n",
      "(DOOR CREAKING)\n",
      "Okay. Okay, please.\n",
      "Please.\n",
      "Please don't hurt me.\n",
      "(BREATHING HEAVILY) Please.\n",
      "Just let me go, okay?<br/>I won't tell anybody.\n",
      "I promise, okay?<br/>Please just let me go. Please.\n",
      "MAN: You need fluids.<br/>You were in shock.\n",
      "What are you going to do to me?\n",
      "I'm going to keep you alive.\n",
      "Work on getting handy with these.\n",
      "My boyfriend was expecting me.\n",
      "He'll send the cops looking.\n",
      "I'm sorry.\n",
      "But no one is looking for you.\n",
      "(CLANGS)\n",
      "(GRUNTS)\n",
      "(GROANS SOFTLY)\n",
      "(GRUNTS)\n",
      "(EXHALES)\n",
      "(SCRAPING)\n",
      "(BREATHING HEAVILY)\n",
      "(SIGHS)\n",
      "(COUGHING)\n",
      "(FIRE ALARM BEEPING)\n",
      "(RAPID FOOTSTEPS APPROACHING)\n",
      "(DOOR OPENS)\n",
      "- (GASPS)<br/>- (GRUNTS)\n",
      "(SCREAMING)\n",
      "(MAN CLEARS THROAT)\n",
      "(GASPS)\n",
      "You've got some fight in you.\n",
      "I can respect that.\n",
      "But don't even think<br/>about trying that again.\n",
      "You're lucky to be here at all.\n",
      "And my generosity only extends so far.\n",
      "Eggs.\n",
      "Toradol to help with your pain.\n",
      "Please.\n",
      "Please, just let me go. Please.\n",
      "There is nowhere to go, Michelle.\n",
      "I looked through your wallet.\n",
      "Given as how I saved your life,<br/>I think that's acceptable.\n",
      "You're lucky to be here at all.\n",
      "What do you mean?\n",
      "I found you, and I saved your life<br/>by bringing you here.\n",
      "I don't understand. I...\n",
      "(SIGHING)\n",
      "There's been an attack.\n",
      "What?\n",
      "An attack. A big one.\n",
      "I'm not sure yet<br/>if it's chemical or nuclear,\n",
      "but down here, we're safe.\n",
      "- And where are we exactly?<br/>- Underneath my farmhouse.\n",
      "Forty miles outside of Lake Charles.\n",
      "I was driving north of here.\n",
      "You were in an accident.\n",
      "You were turned over<br/>on the side of the road.\n",
      "I was driving by and I saw...\n",
      "I saved your life, Michelle.\n",
      "I couldn't just leave you there.\n",
      "(SOFTLY) Okay.\n",
      "Well,\n",
      "thank you so much\n",
      "for saving my life.\n",
      "I guess I should go to a hospital now.\n",
      "You can't leave.\n",
      "An attack means fallout.\n",
      "Which contaminates<br/>the air above ground.\n",
      "(STUTTERS) That's how it works.\n",
      "How long do we have to wait<br/>until it's safe?\n",
      "Depends on the proximity<br/>of the closest blast.\n",
      "One year, maybe two.\n",
      "(HOWARD SIGHS)\n",
      "And that's if we're talking<br/>about weapons that we know of.\n",
      "Russians are developing<br/>some nasty stuff.\n",
      "And if the Martians<br/>finally figured out a way to get here,\n",
      "their weapons will make<br/>the Russkies' look like sticks and stones.\n",
      "Luckily,\n",
      "I've prepared for this.\n",
      "(SOFTLY) Right.\n",
      "Well, I'll need to use your phone then\n",
      "to call my family<br/>and tell them that I'm safe here\n",
      "and make sure they're okay.\n",
      "Michelle, they're not okay.\n",
      "(HOWARD SIGHS)\n",
      "How do you know that?\n",
      "Everyone outside of here is dead.\n",
      "But what about you?<br/>Don't you have a family?\n",
      "(OBJECTS CLATTERING)\n",
      "Who's that?\n",
      "Excuse me.\n",
      "HOWARD: (SHOUTS) What did you do?\n",
      "(LOUD THUDDING)\n",
      "(INDISTINCT SHOUTING)\n",
      "(GASPS)\n",
      "You know what, Michelle?<br/>I'm gonna tell you what I told him.\n",
      "You need to eat, you need to sleep,\n",
      "and you need to start<br/>showing me a little bit of appreciation.\n",
      "My name is Howard, by the way.\n",
      "(DOOR CREAKING)\n",
      "(DOOR LOCKS)\n",
      "(VEHICLE ENGINE STARTS)\n",
      "(VEHICLE DRIVES AWAY)\n",
      "(MUSIC PLAYING IN DISTANCE)\n",
      "- (CLATTERS)<br/>- (GASPS)\n",
      "Oh, God! Oh, shit.\n",
      "I'm sorry. (CHUCKLES)\n",
      "(GROANS SOFTLY)\n",
      "I'm sorry about that.\n",
      "Uh, I didn't mean to scare you.\n",
      "Are you, um...\n",
      "Here. You hungry?\n",
      "So, how are you doing? You okay?\n",
      "- What is this?<br/>- It's a bunker.\n",
      "Your room's a little bit of a fixer-upper,<br/>but at least you got a door.\n",
      "A scary door, but you still got a door.\n",
      "How long have you been down here?\n",
      "Couple of days, I think.\n",
      "You know,<br/>it's actually kind of hard to tell\n",
      "with no windows or sunlight<br/>or anything. (CHUCKLES)\n",
      "(WHISPERS) I mean,<br/>how do we get out of here?\n",
      "Oh, he didn't tell you?\n",
      "About...\n",
      "Getting out of here<br/>is the last thing you want to do.\n",
      "Because the air up there,\n",
      "it's contaminated.\n",
      "HOWARD: I see you've met Emmett.\n",
      "What happened to him?\n",
      "He did that to himself.\n",
      "And his stumbling around<br/>isn't helping anything.\n",
      "What you heard earlier,\n",
      "was him knocking over a shelf<br/>with a whole week's worth of food,\n",
      "which he's sorry for, correct?\n",
      "Totally.\n",
      "Let's go. Bathroom time.\n",
      "(MUSIC CONTINUES PLAYING)\n",
      "HOWARD: This is the common area.\n",
      "Good for R & R.\n",
      "As you can see,<br/>I've planned for a long stay.\n",
      "The aquaponics system<br/>cleans the air and keeps it fresh.\n",
      "This is the living room.<br/>Help yourself to any reading.\n",
      "If you like to watch films,<br/>I have some on DVD and VHS cassette.\n",
      "Just make sure you put them back<br/>in their sleeve when they're done.\n",
      "The kitchen's fully functional.\n",
      "Has an electric stove,<br/>refrigerator, freezer, silverware.\n",
      "And that table's a family heirloom,\n",
      "which means watch your glasses.\n",
      "Always use coasters and placemats.\n",
      "Keep your hands to yourself!\n",
      "Understand?\n",
      "No touching.\n",
      "Take a seat.\n",
      "This way.\n",
      "This is my private space.\n",
      "Off-limits, unless<br/>I give express permission.\n",
      "Go ahead.\n",
      "- I don't need to.<br/>- You will, though.\n",
      "And I've got to pace<br/>these things out, so please.\n",
      "I need privacy.\n",
      "You're welcome to close the curtain.\n",
      "I can't with you standing there.\n",
      "And I can't trust you<br/>not to burn this place down.\n",
      "This is for my own safety.\n",
      "I'm not some pervert! Just go.\n",
      "(SIGHING)\n",
      "HOWARD: Don't flush<br/>unless you've gone!\n",
      "We can't afford wasted flushes.\n",
      "Take a seat. Are you hungry?\n",
      "- Those are Megan's.<br/>- (VESSELS CLANG)\n",
      "She never went anywhere<br/>without two or three of those things.\n",
      "Who's Megan?\n",
      "Megan's not with us anymore.\n",
      "(FAINT WHIRRING)\n",
      "Don't worry. Just the generator.\n",
      "Maybe it's a car outside.\n",
      "(SCOFFS)\n",
      "That's not possible.\n",
      "I heard one earlier. Above my room.\n",
      "If you had heard a car,<br/>the driver would be long dead by now.\n",
      "Well, shouldn't we at least try\n",
      "to call the police or someone<br/>to find out what actually happened?\n",
      "There's no one left to call.\n",
      "See that? Nothing's coming through.\n",
      "(HOWARD EXHALES DEEPLY)\n",
      "You think I sound crazy.\n",
      "I mean, it's amazing. You people...\n",
      "You wear helmets<br/>when you ride your bikes.\n",
      "You have seat belts in your cars.\n",
      "You have alarm systems<br/>to protect your homes.\n",
      "But what do you do<br/>when those alarms go off?\n",
      "\"Crazy\" is building your ark<br/>after the flood has already come!\n",
      "I think maybe it's time<br/>you met Frank and Mildred.\n",
      "(KEYS JANGLING)\n",
      "Closest I could get to an airlock.\n",
      "- (GASPING)<br/>- See?\n",
      "What happened to them?\n",
      "They weren't as lucky as you.\n",
      "It's the air, Michelle.\n",
      "That's what happens<br/>when you get exposed.\n",
      "(CELL PHONE RINGING)\n",
      "(TIRES SQUEALING)\n",
      "(CRASHING)\n",
      "I keep this door sealed at all times.\n",
      "No one comes in or out.\n",
      "EMMETT: Met Frank and Mildred, huh?\n",
      "It's funny, right?\n",
      "The whole world ends\n",
      "and the thing he's most upset about<br/>is a pair of dead pigs?\n",
      "You in need of some reading material?\n",
      "I took all the quizzes. Sorry.\n",
      "But I did learn<br/>how to do a French braid, so,\n",
      "you know, if you want me<br/>to do that for you, just let me know.\n",
      "What do you know about him?\n",
      "He was in the Navy, I know that.\n",
      "I guess he did, uh,<br/>some stuff with satellites.\n",
      "What kind of stuff?\n",
      "Satellite stuff.\n",
      "Well, what brought him out here?\n",
      "I'm not sure.\n",
      "He bought this property a while back.\n",
      "But I never paid much attention\n",
      "till he hired me to help him<br/>get this place set up.\n",
      "The work was entertaining, though,<br/>that's for sure. (CHUCKLES)\n",
      "You know, Howard's like a black belt<br/>in conspiracy theory.\n",
      "Plus, you know, how often<br/>do you get hired to help build\n",
      "a doomsday bunker?\n",
      "So, he didn't\n",
      "kidnap you?\n",
      "(SCOFFS) No!\n",
      "What about your arm?<br/>Weren't you trying to escape?\n",
      "I was trying to get in.\n",
      "I watched Howard build this place,<br/>piece by piece, for years.\n",
      "He was always talking about,\n",
      "you know, possible attacks from.\n",
      "Al Qaeda, Russia, South Korea.\n",
      "You mean North Korea.\n",
      "(STUTTERS) Is that the crazy one?\n",
      "So, yeah, that one.\n",
      "And, uh, you know,<br/>poured all his money into this place.\n",
      "Took to it like his life depended on it.\n",
      "Which, you know,\n",
      "that stays with you.\n",
      "So...\n",
      "(GRUNTS SOFTLY)\n",
      "He told you all this\n",
      "while you're building his bomb shelter.\n",
      "And now he says that\n",
      "the air is contaminated<br/>and everybody's dead.\n",
      "Yeah, I know what you're getting at,<br/>but there's more to it.\n",
      "Howard abducted me.\n",
      "He drove me off the road<br/>and he dragged me here.\n",
      "So, whatever he's telling you<br/>about the air, some big attack,\n",
      "the purpose of this shelter, is a lie.\n",
      "No. No way.\n",
      "The attack, I saw it myself.\n",
      "What do you mean?\n",
      "I was on my way home from work.\n",
      "And it looked like a flash.\n",
      "Bright red.\n",
      "Like an explosion, from way far off.\n",
      "It wasn't like fireworks.\n",
      "Naw, this was more like<br/>something you'd read about in the Bible.\n",
      "So, what, you saw, what,<br/>a flash of light? Lightning?\n",
      "A fire that flared up?\n",
      "I'm not explaining it right.\n",
      "This wasn't like anything I'd ever seen.\n",
      "And so, my first thought<br/>was to come here.\n",
      "And when I got here,<br/>Howard was closing the door.\n",
      "And I could see it,<br/>right there on his face,\n",
      "he knew something was happening.\n",
      "Something bad.\n",
      "And so, I fought my way in.\n",
      "I heard a car.\n",
      "Right here. Above us.\n",
      "You heard someone?\n",
      "Right above us.\n",
      "That isn't possible.\n",
      "- The air is...<br/>- What, contaminated?\n",
      "- How do you know that?<br/>- Because I told him.\n",
      "Dinner's ready.\n",
      "I see you two are getting along.\n",
      "Hmm.\n",
      "How's that sauce?\n",
      "It's fine.\n",
      "As cooks go, I'm okay.\n",
      "Not great, but okay.\n",
      "Megan was a good cook.\n",
      "You'll learn to love cooking.\n",
      "Mmm...\n",
      "EMMETT: Mmm.\n",
      "It's delicious.\n",
      "That's the best sauce I've ever tasted.\n",
      "Are you being funny?\n",
      "No, I mean,\n",
      "considering the alternative,<br/>which is, you know,\n",
      "getting burnt up in a chemical attack,<br/>or nuclear,\n",
      "I'd say being alive and down here would\n",
      "make a fried turd taste pretty good, so...\n",
      "Best damn sauce I ever had.\n",
      "That's not a bad point.\n",
      "And, please,<br/>watch your language at table.\n",
      "Right?\n",
      "You know what I haven't been able to<br/>get out of my head, for some reason?\n",
      "Ever since I got down here.\n",
      "Tattoos.\n",
      "Always wanted one.\n",
      "But I never got any.<br/>'Cause everybody always said,\n",
      "\"No, no, Emmett, you'll never get<br/>a decent job if you do that.\"\n",
      "Whatever.\n",
      "Like that matters now, right?\n",
      "T ell you what,<br/>if I had known this was coming,\n",
      "I would've gotten, like,<br/>50 of them. I swear, man.\n",
      "I would look like a circus freak,<br/>or something.\n",
      "I'd just be covered head to toe.\n",
      "Tattoos all over my...<br/>You know, everywhere. Face?\n",
      "Sure. Right there. Across my forehead.\n",
      "Just my name. \"Emmett.\"\n",
      "Or, you know,\n",
      "- \"Thug Life.\"<br/>- (CHUCKLES)\n",
      "\"YOLO.\"\n",
      "I don't even know what that means,\n",
      "but, you know,<br/>I hear people saying it all the time,\n",
      "so, it must be cool. (CHUCKLES)\n",
      "Hey, what about you, Howard?\n",
      "Anything you wish you'd done?\n",
      "In all honesty, no.\n",
      "No?\n",
      "No crazy nights in Vegas?\n",
      "Maybe, uh, take a pilgrimage to Waco?\n",
      "Everything I wanted to do, I did.\n",
      "I focused on being prepared.\n",
      "And I was.\n",
      "And here we are.\n",
      "EMMETT: Oh, my goodness.\n",
      "Is that Monopoly?\n",
      "There we go!\n",
      "Yeah, that's how we kill the time.\n",
      "What would you say, Howard,<br/>we're gonna be down here,\n",
      "what, like, a year? Two? Maybe?\n",
      "I bet you if we started a game right now,\n",
      "we might even<br/>get halfway finished by the time...\n",
      "Stop talking!\n",
      "(BREATHES DEEPLY)\n",
      "You don't need to make jokes about<br/>how long we're going to be down here\n",
      "when nobody knows<br/>how long that's going to be.\n",
      "Your humor is not funny.\n",
      "I don't appreciate it<br/>while I'm trying to eat,\n",
      "and neither does Michelle.\n",
      "Now, please shut up<br/>and let us eat in peace.\n",
      "Hmm.\n",
      "Emmett?\n",
      "Would you pass me a napkin?\n",
      "Thank you.\n",
      "I know what you're saying.<br/>I never could finish <i>Monopoly.</i>\n",
      "That game really<br/>does take forever, right?\n",
      "For me, though, it was, um...\n",
      "<i>Chutes and Ladders,</i>\n",
      "uh...\n",
      "<i>Sorry!,</i>\n",
      "and <i>Trouble.</i>\n",
      "You know, the thing with the dice<br/>and the thing that you pressed,\n",
      "- what was that called?<br/>- The...\n",
      "BOTH: Pop-O-Matic bubble! Yeah.\n",
      "Did you ever play that, um...<br/>What was that, <i>Operation?</i>\n",
      "- Loved <i>Operation!</i><br/>- I'm telling you, man,\n",
      "I couldn't play that game.\n",
      "- It terrified me.<br/>- Why...\n",
      "That noise that thing would make<br/>when you hit that edge. I mean...\n",
      "- Good Lord!<br/>- (CHUCKLING) It was pretty scary.\n",
      "Could you hand me the salt?\n",
      "Please?\n",
      "Ah, shoot! I'm sorry.\n",
      "I'm gonna need the pepper also.\n",
      "What exactly do you think you're doing?\n",
      "I'm asking for pepper.\n",
      "Like hell you were! What was that?\n",
      "I don't know what you're<br/>talking about, Howard.\n",
      "Are you trying to insult me?\n",
      "Here, in the shelter that I built<br/>that's keeping you alive?\n",
      "You don't think I see what you just did?\n",
      "Is that how you thank me<br/>for saving your life?\n",
      "- Howard, calm down.<br/>- Shut up!\n",
      "Shut up and stay in your seat!\n",
      "Is it?\n",
      "Now, let me tell you,\n",
      "I know what a traitor looks like.\n",
      "Understand?\n",
      "I have shown you nothing,<br/>but generosity and hospitality.\n",
      "I want you to apologize.\n",
      "To tell me you're going to behave.\n",
      "I will.\n",
      "You will what?\n",
      "I'll behave.\n",
      "And I'm so sorry.\n",
      "HOWARD: Sit.\n",
      "(HOWARD SIGHING)\n",
      "Ah...\n",
      "Have to stay hydrated.\n",
      "Hmm.\n",
      "That's easy to forget down here.\n",
      "What's wrong?\n",
      "(SOFTLY) Nothing.\n",
      "Where are my keys?\n",
      "(RUMBLING)\n",
      "(SCREAMS)\n",
      "(BREATHING HEAVILY)\n",
      "- Michelle!<br/>- (GRUNTS)\n",
      "(GROANS)\n",
      "(PANTING)\n",
      "Michelle! Stop!\n",
      "Give me those keys!\n",
      "MICHELLE: Come on.<br/>HOWARD: No! Don't!\n",
      "(MICHELLE PANTING)\n",
      "Stop!\n",
      "No! No, don't! Don't!\n",
      "Don't open that door!\n",
      "(PANTING)\n",
      "There's a car!\n",
      "- There's a car! I see a car!<br/>- HOWARD: No!\n",
      "Here! Here! Here!\n",
      "HOWARD: Michelle,<br/>listen to me, don't do this!\n",
      "- (THUD)<br/>- (GASPS)\n",
      "Oh, God! Thank God!\n",
      "- There's a woman.<br/>- WOMAN: Open the door!\n",
      "(STUTTERING) It's okay.<br/>I just want to come inside.\n",
      "She looks hurt.<br/>She wants me to let her in.\n",
      "Do not let her in!\n",
      "- Look at her face, Michelle!<br/>- No!\n",
      "No, no, no!\n",
      "No!\n",
      "Oh, my God, I'm fine. I really am fine.\n",
      "- No, please, I'm okay.<br/>- There's...\n",
      "It really only...<br/>It only touched me a little. A little!\n",
      "So, could you open the door? Open it!\n",
      "- She's begging me.<br/>- You can't help her!\n",
      "No one can!\n",
      "Let me in. I'll be okay.\n",
      "I'll be okay.<br/>It really hardly touched me at all.\n",
      "- Open the door!<br/>- HOWARD: Don't listen to her!\n",
      "God! Open the door, you bitch!\n",
      "Let me in!\n",
      "Let me in! Let me in!\n",
      "Let me in! Let me in!\n",
      "You!\n",
      "You! You! You!\n",
      "You!\n",
      "(POUNDING ON DOOR)\n",
      "(WHIMPERS)\n",
      "(GRUNTS)\n",
      "(KEYS JANGLING)\n",
      "(DOOR OPENING)\n",
      "(GRUNTS)\n",
      "I know it's hard,\n",
      "realizing they're all gone.\n",
      "The ones you love.\n",
      "I have something to confess to you.\n",
      "(GULPS)\n",
      "I crashed into your car.\n",
      "Your accident\n",
      "was my fault.\n",
      "When I found out about<br/>the incoming attack,\n",
      "I got frantic.\n",
      "I knew I needed to get back here<br/>as soon as possible,\n",
      "so I was driving like a maniac.\n",
      "(SIGHS)\n",
      "I tried to pass you,\n",
      "and\n",
      "I'm the reason you went off the road.\n",
      "I mean, I know I seem<br/>like a sensible guy,\n",
      "but at the time, I wasn't myself.\n",
      "It was an accident, but it was my fault.\n",
      "I was afraid to tell you,\n",
      "and,\n",
      "I'm sorry.\n",
      "Um...\n",
      "You should shower.\n",
      "Even the smallest amount of air\n",
      "that came through the hinges<br/>could be toxic.\n",
      "These were Megan's.\n",
      "If you want.\n",
      "I recognized that woman's car.\n",
      "Her name was Leslie, I think.\n",
      "You knew her?\n",
      "She was a neighbor.\n",
      "Emmett wasn't the only one<br/>who knew about this place.\n",
      "If any others somehow survived,<br/>they could be coming here, too.\n",
      "(GRUNTS SOFTLY)\n",
      "As of Friday, kindness and generosity<br/>are antiquated customs.\n",
      "I'm going to need some stitches.\n",
      "What? You want me to...\n",
      "This is your doing, isn't it?\n",
      "I mean, I don't think I'm really qualified.\n",
      "I'll walk you through it.\n",
      "Here.\n",
      "Have a drink.\n",
      "What is it?\n",
      "Technically, it's vodka.\n",
      "(SLURPING)\n",
      "Ah! It's safe. I distilled it myself.\n",
      "(SLURPS AND COUGHS)\n",
      "I just said I distilled it,\n",
      "I didn't say anything about it<br/>actually tasting good.\n",
      "- Yeah, that's awful.<br/>- You want it on the rocks?\n",
      "Little trick I taught myself<br/>as a young man\n",
      "stationed on a ship<br/>with way too much free time.\n",
      "Every now and again,<br/>if the C.O. was working us too hard,\n",
      "we'd freeze and snap the knob off\n",
      "the bathroom door<br/>while he was still inside.\n",
      "It usually took him an hour or two<br/>to get out.\n",
      "- I'm good.<br/>- Suit yourself. Cheers.\n",
      "Ah...\n",
      "This is clean.\n",
      "All you need to do is stitch.\n",
      "(BREATHING DEEPLY)\n",
      "(WINCES)\n",
      "(HOWARD GASPS)\n",
      "You're doing fine.\n",
      "HOWARD: Some stuff<br/>I grabbed from your car.\n",
      "I didn't have time<br/>to bring in the booze. Sadly.\n",
      "What is all that?\n",
      "I wanted to design clothes.\n",
      "No wonder you were so good<br/>with the stitches.\n",
      "Megan wanted to be an artist.\n",
      "She was your daughter?\n",
      "Yes.\n",
      "She was smart.\n",
      "Loved to read.<br/>The magazines were just for fun.\n",
      "She inhaled books. Anything with Paris.\n",
      "She liked their movies,<br/>their culture, you know.\n",
      "We used to have this little joke.\n",
      "Every once in a while I'd ask her,\n",
      "\"What do you want to be<br/>when you grow up?\"\n",
      "You know what she'd say?\n",
      "\"French.\" (CHUCKLES)\n",
      "Anyway, her mother<br/>turned her against me.\n",
      "Took her off to Chicago.\n",
      "People are strange creatures.\n",
      "You can't always convince them<br/>that safety is in their own best interest.\n",
      "You don't know they're gone.\n",
      "Anyway, at least I tried to help them.\n",
      "- EMMETT: Hey.<br/>- (KNOCKING ON WALL)\n",
      "There was nothing<br/>you could've done for that woman.\n",
      "Even if you let her in,<br/>she still would've died.\n",
      "You asked earlier about regrets?\n",
      "Yeah, I've got some of those.\n",
      "Welcome to the club.\n",
      "(SCOFFS)\n",
      "I mean,<br/>I lived my life in a 40-mile radius.\n",
      "And that was by design.<br/>I made sure that happened.\n",
      "I was so fast in high school,\n",
      "I even managed to outrun<br/>my bad grades.\n",
      "I was All-State track three years in a row.\n",
      "(SNIFFLES AND CLICKS TONGUE)\n",
      "Caught a full ride to Louisiana Tech,<br/>up there in Ruston.\n",
      "(CHUCKLES)\n",
      "I remember I spent<br/>the last two weeks of that summer\n",
      "showing off the bus ticket they sent me<br/>to anybody who'd take a look at it.\n",
      "And then came the night before<br/>I was supposed to leave.\n",
      "And I just got so worried\n",
      "about how bad I was gonna do up there\n",
      "with all those smart kids.\n",
      "So, I went out of my way\n",
      "to get just piss-wasted so bad\n",
      "that I knew there was no chance<br/>I was waking up in the morning.\n",
      "So, I missed the bus.\n",
      "And I didn't buy a ticket for the next one.\n",
      "Or the one after that.\n",
      "Well, if you'd gone,<br/>you might be dead now.\n",
      "(CHUCKLING) Yeah, lucky me, right?\n",
      "Lucky us.\n",
      "(STUTTERS) A few years ago,\n",
      "I was at a hardware store.\n",
      "And there was this little<br/>girl with her dad.\n",
      "And he was in a hurry,<br/>and she wasn't keeping up.\n",
      "So, he kept\n",
      "yanking on her arm.\n",
      "But really hard, you know?\n",
      "Too hard.\n",
      "I know that feeling.\n",
      "When my dad got that way,\n",
      "my brother Colin was always there<br/>to take the worst of it\n",
      "for me.\n",
      "And I thought, seeing this little girl,\n",
      "I thought maybe I could do that\n",
      "for her.\n",
      "But I just kept watching.\n",
      "And they're about to leave,<br/>and I've done nothing.\n",
      "And she slips.\n",
      "And it throws him off-balance,<br/>and he hits her.\n",
      "And I wanted so badly to do something\n",
      "to help her, but...\n",
      "I did what I always do<br/>when things get hard.\n",
      "I just panicked and ran.\n",
      "(SCOFFS)\n",
      "(SNIFFLES)\n",
      "Look, we're here.\n",
      "We're alive.\n",
      "And that means something.\n",
      "It's gotta.\n",
      "(SIGHS)\n",
      "(UPBEAT MUSIC PLAYING)\n",
      "(MAN ULULATING ON TV)\n",
      "(MAN SCREAMING ON TV)\n",
      "- You've got to be kidding me.<br/>- What?\n",
      "We're missing pieces here.\n",
      "Look at this poor cat.<br/>He's been deformed. He's got one eye.\n",
      "He's about to go snorkeling<br/>and everything, too.\n",
      "What are you doing?\n",
      "Ooh!\n",
      "Cornering the market on<br/>post-apocalyptic fashion, huh?\n",
      "Mmm-hmm.\n",
      "You need more axes and chain saws.\n",
      "For what? She has a shotgun.\n",
      "Yeah. But what if up there it's like...\n",
      "What? Lumberjacks?\n",
      "Zombies.\n",
      "Though even Howard doesn't think<br/>that one's plausible.\n",
      "But you should hear his theory<br/>about mutant space worms.\n",
      "(CHUCKLES SOFTLY)\n",
      "(RUMBLING)\n",
      "What is that? Howard!\n",
      "Stay calm. We're okay.\n",
      "(RUMBLING AND CLANGING)\n",
      "- What was that?<br/>- Quiet.\n",
      "That sounds like helicopters.\n",
      "HOWARD: Could be military.\n",
      "But not ours.\n",
      "- EMMETT: How can you tell?<br/>- Fourteen years in the Navy.\n",
      "What's happening up there?\n",
      "My guess?\n",
      "Those flashes that kicked this all off?\n",
      "That was phase one.\n",
      "Take out your opponent's<br/>population centers\n",
      "with big hits, all at once, fast.\n",
      "And then for round two,\n",
      "ground sweeps.\n",
      "A satellite log showed an increase<br/>in coded traffic recently.\n",
      "Possibly extraterrestrial signals.\n",
      "I bet what we just heard<br/>were airborne patrols\n",
      "sent to hunt down<br/>the remaining signs of life.\n",
      "Like us.\n",
      "- (SOFT WHIRRING)<br/>- (ALARM BUZZING)\n",
      "Okay. Oh, boy.\n",
      "That's bad.\n",
      "(GRUNTS)\n",
      "And that's worse.\n",
      "What's up there?\n",
      "Air filtration system.\n",
      "I can't... (GRUNTS)\n",
      "Something blocking the hatch.\n",
      "If we can't get it back on, we're gonna<br/>run out of breathable air fast.\n",
      "(HOWARD GRUNTS)\n",
      "You're the only one<br/>small enough to reach it.\n",
      "Reach what?\n",
      "The filtration system.<br/>Through there, the main duct.\n",
      "Someone needs to get in there<br/>and restart it.\n",
      "Give me a hand.\n",
      "Let me go. She's not gonna<br/>know her way around the unit.\n",
      "You won't fit. Plus your arm.<br/>She'll be fine.\n",
      "Now, to restart the unit,<br/>you just swing the handle off,\n",
      "then on, off, then on.\n",
      "That should do it.\n",
      "And neither of us will be able to go in<br/>and help you if you get stuck.\n",
      "Don't get stuck.\n",
      "(GRUNTING SOFTLY)\n",
      "HOWARD: Michelle!\n",
      "Everything okay up there?\n",
      "It looks like a dead end.\n",
      "HOWARD: That's the incline.\n",
      "Climb up that and you're almost there.\n",
      "Ah, this sucks.\n",
      "(GRUNTING)\n",
      "(WHIRRING)\n",
      "(SIGHING)\n",
      "EMMETT: What's wrong?\n",
      "(WHISPERS) Howard.\n",
      "He lied, he lied about Megan.\n",
      "What do you mean?\n",
      "I think he did something horrible to her.\n",
      "No, his family moved<br/>to Chicago years ago.\n",
      "What's this?\n",
      "Is that blood?\n",
      "Here, come with me.\n",
      "Wait, that... That's not Megan.\n",
      "What do you mean?\n",
      "Yeah. Her name is Brittany.<br/>I remember her.\n",
      "She went to high school<br/>with my little sister.\n",
      "She...\n",
      "She went missing. Two years back.\n",
      "It was on the news and everything.\n",
      "Most people just thought<br/>she skipped town.\n",
      "There was a message up there.\n",
      "It said \"Help.\"\n",
      "It was scratched<br/>on the inside of the window.\n",
      "And this earring...\n",
      "This earring was with it.\n",
      "Did she ever show back up<br/>after she went missing?\n",
      "He said to me... He said to my face,<br/>that this was his daughter.\n",
      "He said this was Megan.\n",
      "- He took her and he killed her.<br/>- (CLATTERING)\n",
      "(WHISPERS) All right, let's just think.\n",
      "Maybe we take away his gun.\n",
      "Tie him up, get him to confess<br/>to whatever it is he's done.\n",
      "Confess to who? The police?\n",
      "Look, like I said,<br/>we can't be the only survivors, right?\n",
      "The woman,<br/>she was able to get around, right?\n",
      "At least a little.\n",
      "Yeah, until she died.\n",
      "Directly above us,<br/>making choking noises.\n",
      "Of all the people to save us...\n",
      "HOWARD: Now, that was<br/>a great example of teamwork.\n",
      "Very well done. I feel like some music.\n",
      "(PUNCHING BUTTONS)\n",
      "Problem solving always puts me<br/>in a musical mood.\n",
      "(CHEERFUL MUSIC PLAYING)\n",
      "Michelle,\n",
      "you should go shower, just in case.\n",
      "Sure.\n",
      "(CHEERFUL MUSIC<br/>CONTINUES PLAYING)\n",
      "(SHOWER RUNNING)\n",
      "I think I might have an idea.\n",
      "\"Ten Better Ways to Style My Bangs\"?\n",
      "No, not the article. This.\n",
      "I think I can only make one,<br/>but it's a start.\n",
      "No kidding.\n",
      "<i>What are you doing up?<br/>I didn't wake you up yet.</i>\n",
      "MAN: <i>Well, I'm fixing your breakfast.</i>\n",
      "<i>One egg over medium.</i>\n",
      "Hey, Howard.\n",
      "What is this?<br/>You re-watching <i>Sixteen Candles?</i>\n",
      "<i>Pretty in Pink.</i>\n",
      "It was one of Megan's favorite movies.<br/>Can I help you with something?\n",
      "No.\n",
      "No, I'm just grabbing some water.\n",
      "(CLEARS THROAT)\n",
      "(MOVIE CONTINUES PLAYING)\n",
      "Say, um...\n",
      "You know, I was just thinking...<br/>(CLEARS THROAT)\n",
      "Not that I'm trying to tell you<br/>how to run this place or anything.\n",
      "I'm just a little curious.\n",
      "Michelle...\n",
      "Say, how close do you think she got<br/>to that air filtration unit?\n",
      "You think she touched it?\n",
      "Yeah, I'm pretty sure she touched it.\n",
      "Well, I know she cleaned up<br/>after and everything,\n",
      "but I'm just thinking,\n",
      "given that unit filters<br/>God knows what through from outside,\n",
      "if she tracked anything back in with her,<br/>it'd be pretty concentrated.\n",
      "And, I mean, it could be<br/>all over the shower and the sink\n",
      "in your bathroom, right now.\n",
      "Anyway, it's just a thought.\n",
      "(MOVIE RESUMES PLAYING)\n",
      "(SHUTS TV OFF)\n",
      "EMMETT: Not bad so far, partner.\n",
      "MICHELLE: If Howard finds this,<br/>he's gonna kill us.\n",
      "All right, so,<br/>we get the gun away from him.\n",
      "All right. We tie him up,<br/>make sure he isn't going anywhere.\n",
      "And then one of us just\n",
      "goes out and looks for help.\n",
      "(SIGHS)\n",
      "(TIMER TICKING)\n",
      "Um...\n",
      "First word.\n",
      "Uh...\n",
      "Tiny.\n",
      "Small. Pygmy. Um...\n",
      "- Little.<br/>- Yep! Mmm-hmm.\n",
      "All right, uh, second word.\n",
      "Ooh! Michelle is a...\n",
      "(STAMMERING)\n",
      "A girl.\n",
      "(STAMMERING CONTINUES)\n",
      "A girl. A child. Um...\n",
      "(GRUNTS)\n",
      "A girl.\n",
      "No, she's older, see, so she is a...\n",
      "\"Little Princess\"?\n",
      "(TIMER RINGING)\n",
      "Um... No, it was woman.\n",
      "Um, Little Women.\n",
      "Wow. Little woman.\n",
      "(SIGHS)\n",
      "(SNORTS)\n",
      "Next time,<br/>try being a little more specific.\n",
      "Let's see here.\n",
      "(TIMER STARTS TICKING)\n",
      "I'm always watching.\n",
      "Always.\n",
      "Um...\n",
      "God.\n",
      "I go wherever I want.\n",
      "Uh...\n",
      "I mean... Well, I don't know.\n",
      "I know what you're doing.<br/>I see what you're doing.\n",
      "Um...\n",
      "I know what you're up to.\n",
      "(STAMMERING) Look, Howard,<br/>I don't know what you're getting at, but...\n",
      "I see you when you're sleeping.\n",
      "I know what you're doing,<br/>and I'm always watching.\n",
      "Always watching! I'm always watching!\n",
      "Santa Claus!\n",
      "(CHUCKLES)\n",
      "You're Santa Claus.\n",
      "(TIMER RINGING)\n",
      "Yeah, Michelle, that's great.\n",
      "Except it was Emmett's turn.\n",
      "Sorry, I just got a little excited.\n",
      "Yeah, well, I'm keeping that point.\n",
      "Totally. You earned it.\n",
      "(TAPPING PEN)\n",
      "(FOOTSTEPS)\n",
      "Emmett?\n",
      "(SHRIEKS)\n",
      "Hey.\n",
      "I need your help with something.\n",
      "Sure.\n",
      "You.\n",
      "On deck.\n",
      "What is this?\n",
      "HOWARD: The barrel.\n",
      "What's in it?\n",
      "Move it into the bathroom.\n",
      "HOWARD: This is perchloric acid.\n",
      "Do either of you know what that is?\n",
      "It's usually produced as a precursor<br/>to ammonium perchlorate,\n",
      "a fuel\n",
      "used for launching<br/>naval satellites into orbit.\n",
      "It's highly corrosive.\n",
      "Dissolves most biological<br/>material on contact.\n",
      "With humans, right down to the bone.\n",
      "Hey, Howard, uh,<br/>what are you showing this to us for?\n",
      "You think I'm an idiot?\n",
      "EMMETT: Uh...<br/>(CHUCKLES NERVOUSLY)\n",
      "Howard, please,<br/>you're gonna have to tell us\n",
      "what it is that you're talking about.\n",
      "I'm talking about<br/>getting rid of some waste.\n",
      "Tell me what you two<br/>were doing with these.\n",
      "You tell me what you two<br/>were planning, right now!\n",
      "Howard, listen, just take it easy.<br/>Take it easy.\n",
      "Howard, come on, please.\n",
      "No. I'm giving you one chance.\n",
      "EMMETT: Hey.\n",
      "Howard, just calm down.\n",
      "One chance to answer<br/>with some dignity, or I swear to God,\n",
      "you're going into this barrel<br/>while you're alive to feel it.\n",
      "It was me.\n",
      "All right? Not her. It's just me.\n",
      "- No, no, no, we...<br/>- Stay out of this, all right?\n",
      "She doesn't have a clue<br/>what you're talking about.\n",
      "I wanted your gun.\n",
      "And so, I was thinking<br/>about making a weapon\n",
      "to get it from you.\n",
      "I want her to respect me<br/>the way that she respects you.\n",
      "I'm not saying that I was right, okay?\n",
      "And I'm sorry.\n",
      "You're sorry?\n",
      "I'm sorry.\n",
      "I accept your apology.\n",
      "(SIGHS)\n",
      "(INAUDIBLE)\n",
      "Listen to me.<br/>You heard him. You heard him.\n",
      "He was making a weapon.<br/>He was gonna hurt us.\n",
      "He was gonna hurt you. And it's okay.\n",
      "This was the way<br/>it was always supposed to be.\n",
      "You're safe.\n",
      "Now it's just you and me.\n",
      "It's okay, you know.\n",
      "You should go to your room now.\n",
      "This next part isn't something<br/>you need to see.\n",
      "Okay.\n",
      "Go on.\n",
      "(SNIFFLES)\n",
      "(SIGHING)\n",
      "HOWARD: Michelle?\n",
      "Hey.\n",
      "I thought we'd change things up tonight\n",
      "and have dessert before dinner.\n",
      "After all,<br/>we can do whatever we want now.\n",
      "Would you like a cone or a bowl?\n",
      "Megan always wanted hers in a bowl.\n",
      "She said the cones were too messy.\n",
      "I know that this isn't<br/>the life you'd prefer,\n",
      "that it isn't easy for<br/>you living down here,\n",
      "but I want us to be a happy family.\n",
      "You and me.\n",
      "The mess is all taken care of.\n",
      "So, just hang loose<br/>and I'll go get dinner started.\n",
      "HOWARD: Michelle?\n",
      "(DOOR OPENS)\n",
      "Everything all right?\n",
      "Yeah, I was just about<br/>to do some reading.\n",
      "Hmm.\n",
      "It's time to set the table.\n",
      "Supper's ready.\n",
      "Yeah.\n",
      "Okay.\n",
      "(METALLIC CLANK)\n",
      "It keeps doing that.<br/>(STUTTERING) I don't know why.\n",
      "Michelle, why is this loose?\n",
      "- Get up.<br/>- Why?\n",
      "- Get off the mattress now!<br/>- (GROANS)\n",
      "(BOTH GRUNTING)\n",
      "Oh, shit!\n",
      "Michelle!\n",
      "Stop! Goddamn it, get back here!\n",
      "(PANTING)\n",
      "(GASPING)\n",
      "HOWARD: You gonna walk out on me?\n",
      "After I saved you and kept you safe,<br/>this is how you repay me.\n",
      "No.\n",
      "This is. (GRUNTS)\n",
      "(SCREAMING)\n",
      "(HOWARD GROANING)\n",
      "(HOWARD WHEEZING)\n",
      "(FIRE ALARM BUZZING)\n",
      "(HOWARD WHEEZING)\n",
      "(GROWLING)\n",
      "(PANTING)\n",
      "(YELPS)\n",
      "HOWARD: Michelle!\n",
      "(GRUNTING)\n",
      "(SCREAMS)\n",
      "(HOWARD GROANING LOUDLY)\n",
      "(GRUNTS)\n",
      "- Stop!<br/>- (YELPS)\n",
      "You don't know what's out there.\n",
      "You can't run from them!\n",
      "(GRUNTS)\n",
      "HOWARD: Stay with me!\n",
      "Let me go!\n",
      "(HOWARD SCREAMS)\n",
      "(MICHELLE COUGHING)\n",
      "(COUGHING)\n",
      "(BREATHING DEEPLY)\n",
      "(HISSING)\n",
      "(GRUNTING)\n",
      "(PANTING)\n",
      "(RIPPING)\n",
      "(GASPS)\n",
      "(GRUNTING)\n",
      "(GRUNTING FRANTICALLY)\n",
      "(BREATHING SHAKILY)\n",
      "(WHIMPERING)\n",
      "(BIRDS SQUAWKING)\n",
      "(CHUCKLES)\n",
      "(BIRDS SQUAWKING)\n",
      "(CHUCKLES)\n",
      "(SIGHS)\n",
      "(WHIRRING IN DISTANCE)\n",
      "(MUFFLED EXPLODING)\n",
      "(MUFFLED EXPLODING)\n",
      "- (EXPLODING)<br/>- (GASPS)\n",
      "Oh, come on.\n",
      "(ALARM WAILING)\n",
      "(PANTING)\n",
      "(ALARM CONTINUES)\n",
      "(SHUDDERING)\n",
      "(RUSTLING)\n",
      "(THUDDING)\n",
      "(GRUNTS)\n",
      "(ALARM CONTINUES WAILING)\n",
      "(SNARLS)\n",
      "Keys, keys, keys.\n",
      "Come on, damn it!\n",
      "- (THUD ON DOOR)<br/>- (GASPS)\n",
      "(THUDDING)\n",
      "(GASPS)\n",
      "(SQUEAKING)\n",
      "(GASPS)\n",
      "(PANTING)\n",
      "Come on. Come on.\n",
      "(ALIEN CHITTERING)\n",
      "(CAR ALARM BEEPS)\n",
      "(GRUNTING)\n",
      "(PANTING)\n",
      "(ALIEN CHITTERING)\n",
      "Help! I'm out here! Help!\n",
      "(RUMBLING)\n",
      "(HISSING)\n",
      "(PANTING)\n",
      "(SNARLING)\n",
      "- (GRUNTING)<br/>- (CHITTERING)\n",
      "(WHIMPERING)\n",
      "(SCREAMS)\n",
      "Oh, God.\n",
      "(RUMBLING)\n",
      "(GRUNTING)\n",
      "(SNARLS)\n",
      "(GROWLS)\n",
      "(EXPLODING)\n",
      "(SCREAMING)\n",
      "(SIGHS)\n",
      "(GLASS CLINKING)\n",
      "(PANTING)\n",
      "Oh, fuck.\n",
      "(ENGINE SPUTTERS)\n",
      "(ENGINE SPUTTERS AND STARTS)\n",
      "(TIRES SCREECHING)\n",
      "(STATIC ON RADIO)\n",
      "FEMALE BROADCASTER: <i>The military<br/>has taken back the southern seaboard.</i>\n",
      "<i>If you are hearing this<br/>and aren't in a safe zone,</i>\n",
      "<i>head north of Baton Rouge.</i>\n",
      "<i>But if you have any medical training<br/>or combat experience,</i>\n",
      "<i>we need help.</i>\n",
      "<i>There are people in Houston.</i>\n",
      "<i>There are survivors at Mercy Hospital.</i>\n",
      "<i>Please help.</i>\n",
      "<i>Repeat. There are people in Houston<br/>who need our help.</i>\n",
      "<i>Come join us.</i>\n",
      "<i>We've taken back<br/>the southern seaboard.</i>\n",
      "<i>And we 're winning.</i>\n",
      "<i>But if you have any medical training<br/>or combat experience,</i>\n",
      "<i>there are survivors...</i>\n",
      "(TIRES SQUEALING)\n",
      "(RUMBLING)\n"
     ]
    }
   ],
   "source": [
    "subs = pysrt.open('Subtitles/10_Cloverfield_lane(2016).srt')\n",
    "print(subs.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297658d",
   "metadata": {},
   "source": [
    "Какие основные проблемы видно в субтитрах:\n",
    "1. HTML-тэги\n",
    "2. Переносы на новую строку \\n\n",
    "3. Знаки препинания, цифры\n",
    "4. Заглавные буквы\n",
    "5. Имена собственные\n",
    "6. Базовые слова, например I, me, we и т.д.\n",
    "7. Разные формы одних и тех же слов (do/did/done и т.д.)\n",
    "8. Теги переводчиков в начале субтитров.\n",
    "\n",
    "Все эти явления либо не несут полезной информации, либо снижают качество обучения нашей модели, при этом увеличивая время работы модели, поэтому в следующем разделе мы постараемся решить некоторые их этих проблем.\n",
    "\n",
    "<br><br> Соберем субтитры в одну таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f513f34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Subtitles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>&lt;font color=\"#ffff80\"&gt;&lt;b&gt;Fixed &amp; Synced by boz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>Hey!\\nI'll be right with you.\\nSo, Cameron. He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>Resync: Xenzai[NEF]\\nRETAIL\\nShould we help hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>- &lt;i&gt;&lt;font color=\"#ffffff\"&gt; Synced and correct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>&lt;i&gt;Oh, I come from a land\\nFrom a faraway plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>While_You_Were_Sleeping(1995)</td>\n",
       "      <td>LUCY: &lt;i&gt;Okay, there are two things that&lt;/i&gt;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Zootopia(2016)</td>\n",
       "      <td>Fear. Treachery Bloodlust.\\nThousands of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>icarus.2017.web.x264-strife</td>\n",
       "      <td>Line drive to right field...\\nSolemnly swear t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>mechanic-resurrection_</td>\n",
       "      <td>Mr. Santos, so good to see you.\\nWe saved your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>z srt23 uk-bun Gullivers.Travels.1939.720p.Blu...</td>\n",
       "      <td>23.976 English\\n&lt;b&gt;Gulliver's Travels (1939)\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Movie  \\\n",
       "0                            10_Cloverfield_lane(2016)   \n",
       "1                     10_things_I_hate_about_you(1999)   \n",
       "2                                 A_knights_tale(2001)   \n",
       "3                                 A_star_is_born(2018)   \n",
       "4                                        Aladdin(1992)   \n",
       "..                                                 ...   \n",
       "274                      While_You_Were_Sleeping(1995)   \n",
       "275                                     Zootopia(2016)   \n",
       "276                        icarus.2017.web.x264-strife   \n",
       "277                             mechanic-resurrection_   \n",
       "278  z srt23 uk-bun Gullivers.Travels.1939.720p.Blu...   \n",
       "\n",
       "                                             Subtitles  \n",
       "0    <font color=\"#ffff80\"><b>Fixed & Synced by boz...  \n",
       "1    Hey!\\nI'll be right with you.\\nSo, Cameron. He...  \n",
       "2    Resync: Xenzai[NEF]\\nRETAIL\\nShould we help hi...  \n",
       "3    - <i><font color=\"#ffffff\"> Synced and correct...  \n",
       "4    <i>Oh, I come from a land\\nFrom a faraway plac...  \n",
       "..                                                 ...  \n",
       "274  LUCY: <i>Okay, there are two things that</i>\\n...  \n",
       "275  Fear. Treachery Bloodlust.\\nThousands of years...  \n",
       "276  Line drive to right field...\\nSolemnly swear t...  \n",
       "277  Mr. Santos, so good to see you.\\nWe saved your...  \n",
       "278  23.976 English\\n<b>Gulliver's Travels (1939)\\n...  \n",
       "\n",
       "[279 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "path = os.path.join('Subtitles/')\n",
    "for file in os.listdir(path):\n",
    "    df = df.append([[file.replace('.srt', ''), pysrt.open(os.path.join(path, file), encoding='latin-1').text]], ignore_index=True)\n",
    "df.columns = ['Movie', 'Subtitles']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c04c8b",
   "metadata": {},
   "source": [
    "Импортируем таблицу movie_titles, в которой содержится классификация уровня английского языка в субтитрах. Сразу избавимся от дубликатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c20415f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Movie   Level\n",
       "0         10_Cloverfield_lane(2016)      B1\n",
       "1  10_things_I_hate_about_you(1999)      B1\n",
       "2              A_knights_tale(2001)      B2\n",
       "3              A_star_is_born(2018)      B2\n",
       "4                     Aladdin(1992)  A2/A2+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movie_titles = pd.read_excel('movies_labels.xlsx').drop('id', axis=1).drop_duplicates(subset=['Movie'])\n",
    "display(movie_titles.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f0cdb",
   "metadata": {},
   "source": [
    "Объединим две таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecbb4d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Subtitles</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>&lt;font color=\"#ffff80\"&gt;&lt;b&gt;Fixed &amp; Synced by boz...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>Hey!\\nI'll be right with you.\\nSo, Cameron. He...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>Resync: Xenzai[NEF]\\nRETAIL\\nShould we help hi...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>- &lt;i&gt;&lt;font color=\"#ffffff\"&gt; Synced and correct...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>&lt;i&gt;Oh, I come from a land\\nFrom a faraway plac...</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>While_You_Were_Sleeping(1995)</td>\n",
       "      <td>LUCY: &lt;i&gt;Okay, there are two things that&lt;/i&gt;\\n...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Zootopia(2016)</td>\n",
       "      <td>Fear. Treachery Bloodlust.\\nThousands of years...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>icarus.2017.web.x264-strife</td>\n",
       "      <td>Line drive to right field...\\nSolemnly swear t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>mechanic-resurrection_</td>\n",
       "      <td>Mr. Santos, so good to see you.\\nWe saved your...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>z srt23 uk-bun Gullivers.Travels.1939.720p.Blu...</td>\n",
       "      <td>23.976 English\\n&lt;b&gt;Gulliver's Travels (1939)\\n...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Movie  \\\n",
       "0                            10_Cloverfield_lane(2016)   \n",
       "1                     10_things_I_hate_about_you(1999)   \n",
       "2                                 A_knights_tale(2001)   \n",
       "3                                 A_star_is_born(2018)   \n",
       "4                                        Aladdin(1992)   \n",
       "..                                                 ...   \n",
       "274                      While_You_Were_Sleeping(1995)   \n",
       "275                                     Zootopia(2016)   \n",
       "276                        icarus.2017.web.x264-strife   \n",
       "277                             mechanic-resurrection_   \n",
       "278  z srt23 uk-bun Gullivers.Travels.1939.720p.Blu...   \n",
       "\n",
       "                                             Subtitles   Level  \n",
       "0    <font color=\"#ffff80\"><b>Fixed & Synced by boz...      B1  \n",
       "1    Hey!\\nI'll be right with you.\\nSo, Cameron. He...      B1  \n",
       "2    Resync: Xenzai[NEF]\\nRETAIL\\nShould we help hi...      B2  \n",
       "3    - <i><font color=\"#ffffff\"> Synced and correct...      B2  \n",
       "4    <i>Oh, I come from a land\\nFrom a faraway plac...  A2/A2+  \n",
       "..                                                 ...     ...  \n",
       "274  LUCY: <i>Okay, there are two things that</i>\\n...      B1  \n",
       "275  Fear. Treachery Bloodlust.\\nThousands of years...      B2  \n",
       "276  Line drive to right field...\\nSolemnly swear t...     NaN  \n",
       "277  Mr. Santos, so good to see you.\\nWe saved your...      B1  \n",
       "278  23.976 English\\n<b>Gulliver's Travels (1939)\\n...      B2  \n",
       "\n",
       "[279 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.merge(df, movie_titles, how='left', on='Movie')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65950433",
   "metadata": {},
   "source": [
    "## Очистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679dea2",
   "metadata": {},
   "source": [
    "Для начала проверим, какие уровни были указаны в таблице movie_titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2bef13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B1', 'B2', 'A2/A2+', 'C1', 'B1, B2', '', 'A2/A2+, B1', 'A2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Level'] = df['Level'].fillna('')\n",
    "df['Level'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b2b1fa",
   "metadata": {},
   "source": [
    "Для некоторых фильмов указаны два или три разных уровня языка. Там, где через слеш указан уровень с плюсом, оставим уровень без плюса, чтобы следовать официальной классификации. Там где указано два уровня без плюса, выберем более высокий, так как он включает в себя слова из более низкого уровня, таким образом модель будет обучаться точнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808a6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Level'] = df['Level'].replace(to_replace=['A2/A2+', 'B1, B2', 'A2/A2+, B1'], value=['A2', 'B2', 'B1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edede091",
   "metadata": {},
   "source": [
    "Приступим к очистке и лемматизации субтитров. Напишем функцию, которая будет проделывать следующие шаги и выдавать очищенный текст:\n",
    "\n",
    "1. Очистка HTML-тэгов\n",
    "2. Удаление \\n, бэкслешей\n",
    "3. Приведение к lowercase\n",
    "4. Удаление знаков препинания\n",
    "5. Проверка на стоп слова\n",
    "6. Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9f457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    clean_text = re.sub('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', ' ', text).replace('\\n', ' ').replace('\\\\', '').lower()\n",
    "    clean_text = re.sub(r'[^\\w\\s]', ' ', clean_text)\n",
    "    clean_text = word_tokenize(clean_text)\n",
    "    clean_text = \" \".join([w for w in clean_text if w not in stop])\n",
    "    clean_text = nlp(clean_text)\n",
    "    return \" \".join([token.lemma_ for token in clean_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ede7ce",
   "metadata": {},
   "source": [
    "Применим функцию к набору субтитров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc4c7518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix sync bozxphd enjoy flick clang drawer close inaudible cell phone ringing ben phone michelle please hang talk okay believe leave michelle come back please say something michelle talk look argument couple fight reason leave everything behind run away going to help michelle please dialtone newscaster detail elsewhere today power still restore many city southern seaboard wake afternoon widespread blackout inclement weather region problem seem link authority call catastrophic power surge crippled traffic area loud crash grunt tire screech scream glass shatter gasping groan horn honk inhale deeply sniff sigh gasp chain rattle breathe heavily grunt groan groan grunt sob chain jangle sob breathe heavily damn clatter grunt rumble footstep approach pant gasp door creak okay okay please please please hurt breathe heavily please let go okay tell anybody promise okay please let go please man need fluid shock go go keep alive work get handy boyfriend expect send cop look sorry one look clang grunt groan softly grunt exhale scrape breathe heavily sigh cough fire alarm beep rapid footstep approach door open gasp grunt scream man clear throat gasp get fight respect even think try lucky generosity extend far egg toradol help pain please please let go please nowhere go michelle look wallet give save life think acceptable lucky mean find save life bring understand sighing attack attack big one sure yet chemical nuclear safe exactly underneath farmhouse forty mile outside lake charles drive north accident turn side road driving see save life michelle leave softly okay well thank much save life guess go hospital leave attack mean fallout contaminate air ground stutter work long wait safe depend proximity close blast one year maybe two howard sigh talk weapon know russian develop nasty stuff martian finally figure way get weapon make russkie look like stick stone luckily prepare softly right well need use phone call family tell safe make sure okay michelle okay howard sigh know everyone outside dead family object clatter excuse howard shout loud thud indistinct shouting gasp know michelle going to tell tell need eat need sleep need start show little bit appreciation name howard way door creak door lock vehicle engine start vehicle drive away music playing distance clatter gasp oh god oh shit sorry chuckle groan softly sorry uh mean scare um hungry okay bunker room little bit fixer upper least get door scary door still get door long couple day think know actually kind hard tell window sunlight anything chuckle whisper mean get oh tell get last thing want air contaminate howard see meet emmett happen stumble around help anything hear early knock shelf whole week worth food sorry correct totally let go bathroom time music continue play howard common area good r r see plan long stay aquaponic system clean air keep fresh living room help read like watch film dvd vhs cassette make sure put back sleeve do kitchen fully functional electric stove refrigerator freezer silverware table family heirloom mean watch glass always use coaster placemat keep hand understand touching take seat way private space limit unless give express permission go ahead need though get pace thing please need privacy welcome close curtain stand trust burn place safety pervert go sigh howard flush unless go afford waste flush take seat hungry megan vessel clang never go anywhere without two three thing megan megan we anymore faint whir worry generator maybe car outside scoff possible hear one early room hear car driver would long dead well least try call police someone find actually happen one left call see nothing come howard exhale deeply think sound crazy mean amazing people wear helmet ride bike seat belt car alarm system protect home alarm go crazy building ark flood already come think maybe time meet frank mildred key jangle close could get airlock gasping see happen lucky air michelle happen get expose cell phone ring tire squeal crash keep door seal time one come emmett meet frank mildred huh funny right whole world end thing upset pair dead pig need read material take quiz sorry learn french braid know want let know know navy know guess uh stuff satellite kind stuff satellite stuff well bring sure buy property back never pay much attention till hire help get place set work entertain though sure chuckle know howard like black belt conspiracy theory plus know often get hire help build doomsday bunker kidnap scoff arm try escape try get watch howard build place piece piece year always talk know possible attack al qaeda russia south korea mean north korea stutter crazy one yeah one uh know pour money place take like life depend know stay grunt softly tell building bomb shelter say air contaminate everybody dead yeah know get howard abducted drive road drag whatever tell air big attack purpose shelter lie way attack see mean way home work look like flash bright red like explosion way far like firework naw like something read bible see flash light lightning fire flare explain right like anything ever see first thought come get howard closing door could see right face know something happen something bad fight way hear car right we hear someone right we possible air contaminate know tell dinner ready see two get along hmm sauce fine cook go okay great okay megan good cook learn love cooking mmm emmett mmm delicious good sauce ever taste funny mean consider alternative know get burn chemical attack nuclear say alive would make fried turd taste pretty good good damn sauce ever bad point please watch language table right know able get head reason ever since get tattoos always want one never get cause everybody always say emmett never get decent job whatever like matter right ell know coming would get like 50 swear man would look like circus freak something cover head toe tattoo know everywhere face sure right across forehead name emmett know thug life chuckle yolo even know mean know hear people say time must cool chuckle hey howard anything wish do honesty crazy night vegas maybe uh take pilgrimage waco everything want focus prepared emmett oh goodness monopoly go yeah kill time would say howard going to like year two maybe bet start game right might even get halfway finish time stop talk breathe deeply need make joke long go nobody know long go humor funny appreciate try eat neither michelle please shut let we eat peace hmm emmett would pass napkin thank know say never could finish monopoly game really take forever right though um chute ladder uh sorry trouble know thing dice thing press call pop matic bubble yeah ever play um operation love operation tell man play game terrified noise thing would make hit edge mean good lord chuckle pretty scary could hand salt please ah shoot sorry going to need pepper also exactly think ask pepper like hell know talk howard try insult shelter build keep alive think see thank save life howard calm shut shut stay seat let tell know traitor look like understand show nothing generosity hospitality want apologize tell go behave behave sorry howard sit howard sigh ah stay hydrated hmm easy forget wrong softly nothing key rumble scream breathe heavily michelle grunt groan pant michelle stop give key michelle come howard michelle pant stop open door pant car car see car howard howard michelle listen thud gasp oh god thank god woman woman open door stutter okay want come inside look hurt want let let look face michelle oh god fine really fine please okay really touch little little could open door open begging help one let okay okay really hardly touch open door howard listen god open door bitch let let let let let pound door whimper grunt key jangle door opening grunt know hard realize go one love something confess gulps crash car accident fault find incoming attack get frantic knew need get back soon possible drive like maniac sigh try pass reason go road mean know seem like sensible guy time accident fault afraid tell sorry um shower even small amount air come hinge could toxic megan want recognize woman car name leslie think know neighbor emmett one know place other somehow survive could come grunt softly friday kindness generosity antiquate custom go need stitch want mean think really qualified walk drink technically vodka slurp ah safe distil slurps cough say distilled say anything actually taste good yeah awful want rock little trick teach young man station ship way much free time every c work we hard freeze snap knob bathroom door still inside usually take hour two get good suit cheer ah clean need stitch breathing deeply wince howard gasp fine howard stuff grab car time bring booze sadly want design clothe wonder good stitch megan want artist daughter yes smart love read magazine fun inhaled book anything paris like movie culture know use little joke every ask want grow know say french chuckle anyway mother turn take chicago people strange creature always convince safety good interest know go anyway least try help emmett hey knock wall nothing could do woman even let still would die ask early regret yeah get welcome club scoff mean live life 40 mile radius design make sure happen fast high school even manage outrun bad grade state track three year row sniffle click tongue catch full ride louisiana tech ruston chuckle remember spend last two week summer show bus ticket send anybody take look come night suppose leave get worry bad going to smart kid go way get piss waste bad knew chance wake morning miss bus buy ticket next one one well go might dead chuckle yeah lucky right lucky we stutter year ago hardware store little girl dad hurry keep keep yank arm really hard know hard know feel dad get way brother colin always take bad thought see little girl think maybe could keep watch leave do nothing slip throw balance hit want badly something help always thing get hard panicked ran scoff sniffle look alive mean something got to sigh upbeat music play man ululate tv man scream tv get kid miss piece look poor cat deform get one eye go snorkel everything ooh corner market post apocalyptic fashion huh mmm hmm need axis chain saw shotgun yeah like lumberjack zombie though even howard think one plausible hear theory mutant space worm chuckle softly rumble howard stay calm okay rumble clang quiet sound like helicopter howard could military emmett tell fourteen year navy happen guess flash kick phase one take opponent population center big hit fast round two ground sweep satellite log show increase code traffic recently possibly extraterrestrial signal bet hear airborne patrol send hunt remain sign life like we soft whir alarm buzzing okay oh boy bad grunt bad air filtration system grunt something block hatch get back going to run breathable air fast howard grunt one small enough reach reach filtration system main duct someone need get restart give hand let go going to know way around unit fit plus arm fine restart unit swing handle neither we able go help get stuck get stick grunt softly howard michelle everything okay look like dead end howard incline climb almost ah suck grunt whir sighing emmett wrong whisper howard lie lie megan mean think something horrible family move chicago year ago blood come wait megan mean yeah name brittany remember go high school little sister go miss two year back news everything people think skip town message say help scratch inside window earring earring ever show back go miss say say face daughter say megan take kill clattering whisper right let think maybe take away gun tie get confess whatever do confess police look like say survivor right woman able get around right least little yeah die directly we make choke noise people save we howard great example teamwork well do feel like music punching button problem solve always put musical mood cheerful music playing michelle go shower case sure cheerful music continue play shower run think might idea ten well way style bang article think make one start kid wake yet man well fix breakfast one egg medium hey howard watch sixteen candle pretty pink one megan favorite movie help something grab water clear throat movie continue play say um know thinking clear throat try tell run place anything little curious michelle say close think get air filtration unit think touch yeah pretty sure touch well know clean everything think give unit filter god know outside track anything back pretty concentrated mean could shower sink bathroom right anyway think movie resume play shut tv emmett bad far partner michelle howard find going to kill we right get gun away right tie make sure go anywhere one we go look help sigh timer tick um first word uh tiny small pygmy um little yep mmm hmm right uh second word ooh michelle stammer girl stammering continue girl child um grunt girl old see little princess timer ring um woman um little woman wow little woman sigh snort next time try little specific let see timer start tick always watch always um god go wherever want uh mean well know know see um know stammer look howard know get see sleep know always watch always watch always watch santa claus chuckle santa claus timer ring yeah michelle great except emmett turn sorry get little excited yeah well keep point totally earn tap pen footstep emmett shriek hey need help something sure deck howard barrel move bathroom howard perchloric acid either know usually produce precursor ammonium perchlorate fuel use launch naval satellite orbit highly corrosive dissolve biological material contact human right bone hey howard uh show we think idiot emmett uh chuckle nervously howard please going to tell we talk talk get rid waste tell two tell two planning right howard listen take easy take easy howard come please give one chance emmett hey howard calm one chance answer dignity swear god go barrel alive feel right stay right clue talk want gun thinking make weapon get want respect way respect say right okay sorry sorry sorry accept apology sigh inaudible listen hear hear make weapon going to hurt we going to hurt okay way always suppose safe okay know go room next part something need see okay go sniffle sigh howard michelle hey think change thing tonight dessert dinner whatever want would like cone bowl megan always want bowl say cone messy know life prefer easy living want we happy family mess take care hang loose go get dinner start howard michelle door open everything right yeah read hmm time set table supper ready yeah okay metallic clank keep stutter know michelle loose get get mattress groan grunt oh shit michelle stop goddamn get back pant gasp howard going to walk save keep safe repay grunt scream howard groan howard wheeze fire alarm buzz howard wheeze growl pant yelps howard michelle grunt scream howard groan loudly grunt stop yelp know run grunt howard stay let go howard scream michelle cough cough breathe deeply hiss grunt pant rip gasp grunt grunt frantically breathe shakily whimper bird squawk chuckle bird squawk chuckle sigh whir distance muffle explode muffle explode explode gasp oh come alarm wail pant alarm continue shudder rustle thud grunt alarm continue wail snarl key key key come damn thud door gasp thud gasp squeak gasp pant come come alien chitter car alarm beep grunt pant alien chitter help help rumble hiss pant snarl grunt chitter whimpering scream oh god rumble grunt snarl growl explode scream sigh glass clink pant oh fuck engine sputter engine sputter start tire screech static radio female broadcaster military take back southern seaboard hear safe zone head north baton rouge medical training combat experience need help people houston survivor mercy hospital please help repeat people houston need help come join we take back southern seaboard win medical training combat experience survivor tire squeal rumble\n"
     ]
    }
   ],
   "source": [
    "df['Subtitles'] = df['Subtitles'].apply(clean)\n",
    "print(df.loc[0, 'Subtitles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca5708",
   "metadata": {},
   "source": [
    "Соберем финальный датасет для анализа, в котором будет указан числовой уровень языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69fca4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Subtitles</th>\n",
       "      <th>Level</th>\n",
       "      <th>level_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>fix sync bozxphd enjoy flick clang drawer clos...</td>\n",
       "      <td>B1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>hey right cameron go nine school 1 0 year army...</td>\n",
       "      <td>B1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>resync xenzai nef retail help due list two min...</td>\n",
       "      <td>B2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>sync corrected mrcjnthn get â ª black eye open...</td>\n",
       "      <td>B2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>oh come land faraway place caravan camel roam ...</td>\n",
       "      <td>A2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>We_are_the_Millers(2013)</td>\n",
       "      <td>oh god full double rainbow way across sky whoa...</td>\n",
       "      <td>B1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>While_You_Were_Sleeping(1995)</td>\n",
       "      <td>lucy okay two thing remember childhood remembe...</td>\n",
       "      <td>B1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Zootopia(2016)</td>\n",
       "      <td>fear treachery bloodlust thousand year ago for...</td>\n",
       "      <td>B2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>mechanic-resurrection_</td>\n",
       "      <td>mr santos good see save usual table mr santo t...</td>\n",
       "      <td>B1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>z srt23 uk-bun Gullivers.Travels.1939.720p.Blu...</td>\n",
       "      <td>23 976 english gulliver travel 1939 base jonat...</td>\n",
       "      <td>B2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Movie  \\\n",
       "0                            10_Cloverfield_lane(2016)   \n",
       "1                     10_things_I_hate_about_you(1999)   \n",
       "2                                 A_knights_tale(2001)   \n",
       "3                                 A_star_is_born(2018)   \n",
       "4                                        Aladdin(1992)   \n",
       "..                                                 ...   \n",
       "223                           We_are_the_Millers(2013)   \n",
       "224                      While_You_Were_Sleeping(1995)   \n",
       "225                                     Zootopia(2016)   \n",
       "226                             mechanic-resurrection_   \n",
       "227  z srt23 uk-bun Gullivers.Travels.1939.720p.Blu...   \n",
       "\n",
       "                                             Subtitles Level  level_numeric  \n",
       "0    fix sync bozxphd enjoy flick clang drawer clos...    B1              3  \n",
       "1    hey right cameron go nine school 1 0 year army...    B1              3  \n",
       "2    resync xenzai nef retail help due list two min...    B2              4  \n",
       "3    sync corrected mrcjnthn get â ª black eye open...    B2              4  \n",
       "4    oh come land faraway place caravan camel roam ...    A2              2  \n",
       "..                                                 ...   ...            ...  \n",
       "223  oh god full double rainbow way across sky whoa...    B1              3  \n",
       "224  lucy okay two thing remember childhood remembe...    B1              3  \n",
       "225  fear treachery bloodlust thousand year ago for...    B2              4  \n",
       "226  mr santos good see save usual table mr santo t...    B1              3  \n",
       "227  23 976 english gulliver travel 1939 base jonat...    B2              4  \n",
       "\n",
       "[228 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_final = df.drop(index=df.loc[df['Level'] == ''].index).reset_index(drop=True)\n",
    "df_final['level_numeric'] = df_final['Level'].map(levels)\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740ca5d",
   "metadata": {},
   "source": [
    "## Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442217a4",
   "metadata": {},
   "source": [
    "Разделим выборку на обучающую и валидационную. Так как датасет небольшой, то тестовую выборку создавать не будем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9000824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(171,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = df_final['Subtitles']\n",
    "target = df_final['level_numeric']\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345\n",
    ")\n",
    "\n",
    "display(features_train.shape)\n",
    "display(target_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040fd33",
   "metadata": {},
   "source": [
    "Рассчитаем частоту tf-idf для всего набора субтитров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9088714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.02972372 0.         ... 0.         0.         0.        ]\n",
      " [0.01867786 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.0063969  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "docs = features_train.values\n",
    "\n",
    "tfidf_v = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "print(tfidf_v.fit_transform(docs).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4c6ce",
   "metadata": {},
   "source": [
    "Соберем пайплайн и подберем лучшие параметры с помощью GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2849b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'lr__solver': ['liblinear', 'lbfgs'],\n",
    "               'lr__penalty': ['l1', 'l2'],\n",
    "               'lr__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf_v),\n",
    "                     ('lr', LogisticRegression(random_state=12345))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=3,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0915c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV 1/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.3s\n",
      "[CV 2/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.3s\n",
      "[CV 3/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.3s\n",
      "[CV 4/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.3s\n",
      "[CV 5/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.3s\n",
      "[CV 1/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.3s\n",
      "[CV 2/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.2s\n",
      "[CV 3/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.2s\n",
      "[CV 4/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.1s\n",
      "[CV 5/5] END lr__C=1.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n",
      "[CV 3/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=1.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.7s\n",
      "[CV 1/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   3.7s\n",
      "[CV 2/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   6.0s\n",
      "[CV 3/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   4.6s\n",
      "[CV 4/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   4.9s\n",
      "[CV 5/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   5.5s\n",
      "[CV 1/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  15.0s\n",
      "[CV 2/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  17.2s\n",
      "[CV 3/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  20.1s\n",
      "[CV 4/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  14.9s\n",
      "[CV 5/5] END lr__C=1.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  17.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  31.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  32.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  31.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  30.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  34.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  34.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  43.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  39.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  39.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=1.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  39.4s\n",
      "[CV 1/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n",
      "[CV 2/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n",
      "[CV 3/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n",
      "[CV 4/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n",
      "[CV 5/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n",
      "[CV 1/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.6s\n",
      "[CV 2/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.2s\n",
      "[CV 3/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.1s\n",
      "[CV 4/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.4s\n",
      "[CV 5/5] END lr__C=10.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   8.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   8.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=10.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.5s\n",
      "[CV 1/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   4.1s\n",
      "[CV 2/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   7.1s\n",
      "[CV 3/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   5.3s\n",
      "[CV 4/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   4.7s\n",
      "[CV 5/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  11.3s\n",
      "[CV 1/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  12.4s\n",
      "[CV 2/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  12.0s\n",
      "[CV 3/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  13.3s\n",
      "[CV 4/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  11.6s\n",
      "[CV 5/5] END lr__C=10.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  30.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  30.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  30.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  37.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  44.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  52.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  35.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  33.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  41.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=10.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  42.2s\n",
      "[CV 1/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.5s\n",
      "[CV 2/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.5s\n",
      "[CV 3/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n",
      "[CV 4/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n",
      "[CV 5/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.4s\n",
      "[CV 1/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.4s\n",
      "[CV 2/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.2s\n",
      "[CV 3/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.1s\n",
      "[CV 4/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.5s\n",
      "[CV 5/5] END lr__C=100.0, lr__penalty=l1, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   9.0s\n",
      "[CV 1/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n",
      "[CV 3/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n",
      "[CV 5/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=100.0, lr__penalty=l1, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=   7.6s\n",
      "[CV 1/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   4.6s\n",
      "[CV 2/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   3.8s\n",
      "[CV 3/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   3.7s\n",
      "[CV 4/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   4.0s\n",
      "[CV 5/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=   4.2s\n",
      "[CV 1/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  17.1s\n",
      "[CV 2/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  12.7s\n",
      "[CV 3/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  15.7s\n",
      "[CV 4/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  11.8s\n",
      "[CV 5/5] END lr__C=100.0, lr__penalty=l2, lr__solver=liblinear, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  11.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  23.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  24.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  27.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  30.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer at 0x7fe942290f70>, vect__use_idf=False; total time=  28.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  32.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  36.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  42.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  31.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END lr__C=100.0, lr__penalty=l2, lr__solver=lbfgs, vect__ngram_range=(1, 1), vect__norm=None, vect__stop_words=None, vect__tokenizer=<function tokenizer_porter at 0x7fe942293af0>, vect__use_idf=False; total time=  31.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.64302521 0.64302521        nan        nan 0.69563025 0.6897479\n",
      " 0.68991597 0.67226891 0.70184874 0.71327731        nan        nan\n",
      " 0.6897479  0.69563025 0.68991597 0.69596639 0.69563025 0.71310924\n",
      "        nan        nan 0.70134454 0.6897479  0.70739496 0.67226891]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(random_state=12345))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'lr__C': [1.0, 10.0, 100.0],\n",
       "                          'lr__penalty': ['l1', 'l2'],\n",
       "                          'lr__solver': ['liblinear', 'lbfgs'],\n",
       "                          'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fe942290f70>,\n",
       "                                              <function tokenizer_porter at 0x7fe942293af0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(features_train.values, target_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b83c57",
   "metadata": {},
   "source": [
    "Выведем на экран лучшие параметры и результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a077084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'lr__C': 10.0, 'lr__penalty': 'l1', 'lr__solver': 'liblinear', 'vect__ngram_range': (1, 1), 'vect__norm': None, 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x7fe942293af0>, 'vect__use_idf': False} \n",
      "CV Accuracy: 0.713\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c21ee",
   "metadata": {},
   "source": [
    "Точность на валидационной выборке 0.526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b49e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.526\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(features_valid, target_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e613c",
   "metadata": {},
   "source": [
    "Выведем на экран предсказания модели и сравним с реальным уровнем английского:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5348e600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subtitles</th>\n",
       "      <th>level_numeric</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>harvey read anything going to cause trouble gu...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>happy birthday happy birthday hell end fuck bi...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>telegraph machine beep train whistle blow tele...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>last june see emily davison crush death beneat...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>harvey reading sutter three year guy one slipp...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>think think name go help raise like give somet...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>resync lututkanan subscene idea argue speak en...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mr bates come morning say would quite thing do...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>common error ocr capitalization issue fix tron...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>hammer open tomorrow afternoon well let get pa...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>upon time hide heart france handsome young pri...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>create encode bokutox www yify torrent com goo...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bridget begin new year day thirty second year ...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tale unprejudiced heart change valley forever ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>america irradiate wasteiand within lie city ou...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Subtitles  level_numeric  \\\n",
       "30  harvey read anything going to cause trouble gu...              4   \n",
       "20  happy birthday happy birthday hell end fuck bi...              5   \n",
       "47  telegraph machine beep train whistle blow tele...              5   \n",
       "52  last june see emily davison crush death beneat...              5   \n",
       "33  harvey reading sutter three year guy one slipp...              4   \n",
       "40  think think name go help raise like give somet...              4   \n",
       "16  resync lututkanan subscene idea argue speak en...              4   \n",
       "49  mr bates come morning say would quite thing do...              5   \n",
       "27  common error ocr capitalization issue fix tron...              4   \n",
       "50  hammer open tomorrow afternoon well let get pa...              5   \n",
       "14  upon time hide heart france handsome young pri...              4   \n",
       "17  create encode bokutox www yify torrent com goo...              4   \n",
       "19  bridget begin new year day thirty second year ...              4   \n",
       "10  tale unprejudiced heart change valley forever ...              2   \n",
       "54  america irradiate wasteiand within lie city ou...              2   \n",
       "\n",
       "    predictions  \n",
       "30            4  \n",
       "20            5  \n",
       "47            4  \n",
       "52            3  \n",
       "33            4  \n",
       "40            3  \n",
       "16            3  \n",
       "49            3  \n",
       "27            4  \n",
       "50            4  \n",
       "14            3  \n",
       "17            4  \n",
       "19            3  \n",
       "10            2  \n",
       "54            3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = features_valid.to_frame().merge(target_valid.to_frame(), left_index=True, right_index=True).merge(pd.DataFrame(clf.predict(features_valid), columns=['predictions']), left_index=True, right_index=True)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7dc46",
   "metadata": {},
   "source": [
    "Модель в основном занижает результат, подробнее о возможных причинах в разделе ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e84c2",
   "metadata": {},
   "source": [
    "## Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecbcb84",
   "metadata": {},
   "source": [
    "Итак, мы построили модель, которая может предсказать уровень английского языка в фильме по субтитрам.\n",
    "<br>Точность итоговой модели на валидационной выборке 0.526, а если сравнить предсказания с ответами, то видно, что в основном модель занижает результаты.\n",
    "<br>Возможные причины такого результата:\n",
    "1. Малый объем выборки. Увеличение выборки позволит расширить общий словарь и сделать резльтаты более точными.\n",
    "2. Недостаточная очистка субтитров. В идеале нужно избавиться от имен собственных, эмозди, цифр, тегов создателей субтитров и прочих вещей, не добавляющих смысловой нагрузки для обучения.\n",
    "3. Орфографические ошибки в словах.\n",
    "\n",
    "Возможные дальнейшие усовершенствования модели:\n",
    "1. Увеличить выборку. Для этого нужно найти субтитры с уже обозначенным уровнем английского языка или придумать систему математического расчета уровня языка.\n",
    "2. Усовершенствовать очистку субтитров. Можно попробовать использовать словарь английских слов и отбрасывать все значения лемматизированных слов, которых в нем нет.\n",
    "3. Отбрасывать первый субтитр. Часто он содержит тэг человека, создавшего субтитры. Это вряд ли снизит общий словарь полезных слов, но поможет избавиться от \"мусора\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3af33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 131,
    "start_time": "2023-07-08T16:17:25.547Z"
   },
   {
    "duration": 85,
    "start_time": "2023-07-08T16:18:25.232Z"
   },
   {
    "duration": 64,
    "start_time": "2023-07-08T16:21:27.507Z"
   },
   {
    "duration": 82,
    "start_time": "2023-07-08T16:25:08.698Z"
   },
   {
    "duration": 326,
    "start_time": "2023-07-08T16:25:38.869Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-08T16:26:14.101Z"
   },
   {
    "duration": 438,
    "start_time": "2023-07-08T16:26:41.227Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-08T16:26:44.214Z"
   },
   {
    "duration": 74,
    "start_time": "2023-07-08T16:27:51.903Z"
   },
   {
    "duration": 3500,
    "start_time": "2023-07-08T16:28:45.024Z"
   },
   {
    "duration": 6244,
    "start_time": "2023-07-08T16:28:57.276Z"
   },
   {
    "duration": 74,
    "start_time": "2023-07-08T16:34:39.442Z"
   },
   {
    "duration": 412,
    "start_time": "2023-07-08T16:34:51.504Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-08T16:34:51.919Z"
   },
   {
    "duration": 99,
    "start_time": "2023-07-08T16:34:51.932Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-08T16:34:52.034Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-08T16:34:52.036Z"
   },
   {
    "duration": 2703,
    "start_time": "2023-07-08T16:34:56.777Z"
   },
   {
    "duration": 128,
    "start_time": "2023-07-08T16:35:01.278Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-08T16:35:14.798Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-08T16:35:29.122Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-08T16:37:41.741Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-08T16:38:40.892Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-08T16:38:58.420Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-08T16:39:25.606Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-08T16:40:42.014Z"
   },
   {
    "duration": 4420,
    "start_time": "2023-07-08T16:43:10.603Z"
   },
   {
    "duration": 33,
    "start_time": "2023-07-08T16:44:00.203Z"
   },
   {
    "duration": 80,
    "start_time": "2023-07-08T16:45:36.117Z"
   },
   {
    "duration": 120,
    "start_time": "2023-07-08T16:49:40.102Z"
   },
   {
    "duration": 266,
    "start_time": "2023-07-08T16:50:04.015Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-08T16:50:31.434Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-08T16:51:01.505Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-08T16:51:19.611Z"
   },
   {
    "duration": 100,
    "start_time": "2023-07-08T16:51:42.347Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-08T16:51:59.764Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-08T16:52:13.158Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-08T16:52:47.447Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-08T16:52:56.408Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-08T16:53:01.007Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-08T16:54:23.431Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-08T16:54:32.171Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T08:54:23.910Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T08:54:43.917Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T08:55:38.641Z"
   },
   {
    "duration": 262,
    "start_time": "2023-07-09T08:55:56.033Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T08:59:38.829Z"
   },
   {
    "duration": 49,
    "start_time": "2023-07-09T08:59:49.276Z"
   },
   {
    "duration": 61,
    "start_time": "2023-07-09T09:01:03.554Z"
   },
   {
    "duration": 40,
    "start_time": "2023-07-09T09:02:28.022Z"
   },
   {
    "duration": 130,
    "start_time": "2023-07-09T09:02:58.003Z"
   },
   {
    "duration": 74,
    "start_time": "2023-07-09T09:03:10.624Z"
   },
   {
    "duration": 196,
    "start_time": "2023-07-09T09:03:34.764Z"
   },
   {
    "duration": 168,
    "start_time": "2023-07-09T09:03:42.980Z"
   },
   {
    "duration": 31190,
    "start_time": "2023-07-09T09:05:08.724Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T09:06:11.564Z"
   },
   {
    "duration": 259,
    "start_time": "2023-07-09T09:06:39.672Z"
   },
   {
    "duration": 397,
    "start_time": "2023-07-09T09:08:02.012Z"
   },
   {
    "duration": 58,
    "start_time": "2023-07-09T09:08:05.924Z"
   },
   {
    "duration": 382,
    "start_time": "2023-07-09T09:08:33.168Z"
   },
   {
    "duration": 17,
    "start_time": "2023-07-09T09:09:18.405Z"
   },
   {
    "duration": 33751,
    "start_time": "2023-07-09T09:10:06.367Z"
   },
   {
    "duration": 377,
    "start_time": "2023-07-09T09:10:55.823Z"
   },
   {
    "duration": 324,
    "start_time": "2023-07-09T09:11:26.622Z"
   },
   {
    "duration": 102,
    "start_time": "2023-07-09T09:12:56.306Z"
   },
   {
    "duration": 95,
    "start_time": "2023-07-09T09:13:21.489Z"
   },
   {
    "duration": 106,
    "start_time": "2023-07-09T09:13:46.733Z"
   },
   {
    "duration": 126,
    "start_time": "2023-07-09T09:14:22.878Z"
   },
   {
    "duration": 365,
    "start_time": "2023-07-09T09:18:57.805Z"
   },
   {
    "duration": 122,
    "start_time": "2023-07-09T09:19:00.329Z"
   },
   {
    "duration": 37192,
    "start_time": "2023-07-09T09:19:02.767Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-09T09:20:07.096Z"
   },
   {
    "duration": 432,
    "start_time": "2023-07-09T09:21:39.459Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-09T09:21:39.893Z"
   },
   {
    "duration": 2128,
    "start_time": "2023-07-09T09:21:39.906Z"
   },
   {
    "duration": 73,
    "start_time": "2023-07-09T09:21:42.037Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T09:21:42.112Z"
   },
   {
    "duration": 3217,
    "start_time": "2023-07-09T09:21:42.118Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T09:21:45.337Z"
   },
   {
    "duration": 30663,
    "start_time": "2023-07-09T09:25:41.290Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T09:26:19.596Z"
   },
   {
    "duration": 13230,
    "start_time": "2023-07-09T09:30:41.225Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-09T09:31:26.340Z"
   },
   {
    "duration": 585,
    "start_time": "2023-07-09T09:32:57.548Z"
   },
   {
    "duration": 84,
    "start_time": "2023-07-09T09:33:14.572Z"
   },
   {
    "duration": 78,
    "start_time": "2023-07-09T09:33:33.057Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-09T09:35:42.728Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T09:36:51.967Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T09:39:07.543Z"
   },
   {
    "duration": 401,
    "start_time": "2023-07-09T09:41:13.957Z"
   },
   {
    "duration": 1996,
    "start_time": "2023-07-09T09:41:14.360Z"
   },
   {
    "duration": 60,
    "start_time": "2023-07-09T09:41:16.358Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T09:41:16.420Z"
   },
   {
    "duration": 13481,
    "start_time": "2023-07-09T09:41:16.425Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T09:41:29.908Z"
   },
   {
    "duration": 208,
    "start_time": "2023-07-09T09:41:29.929Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T09:41:30.139Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T09:57:33.218Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T09:57:43.422Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T10:00:06.808Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T10:00:28.430Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T10:00:40.509Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T10:03:02.416Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T10:03:14.414Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T11:20:17.414Z"
   },
   {
    "duration": 149,
    "start_time": "2023-07-09T11:28:16.270Z"
   },
   {
    "duration": 78,
    "start_time": "2023-07-09T11:28:43.702Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T11:29:02.761Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T11:29:11.518Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T11:29:16.858Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:30:09.514Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:30:41.266Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:33:27.102Z"
   },
   {
    "duration": 125,
    "start_time": "2023-07-09T11:33:59.865Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T11:34:47.253Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-09T11:36:25.349Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T11:36:47.033Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:37:19.490Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:38:19.333Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T11:38:43.388Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T11:40:14.969Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T11:40:20.189Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T11:41:05.194Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-09T11:41:15.593Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T11:42:16.833Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:42:49.968Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T11:43:28.713Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:47:58.119Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-09T11:50:15.700Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T11:51:12.934Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T11:52:11.072Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:52:33.803Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:53:13.263Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T11:53:36.818Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:54:29.047Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:54:32.015Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:54:52.999Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T11:54:54.911Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T11:55:00.451Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:55:16.375Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:55:18.503Z"
   },
   {
    "duration": 921,
    "start_time": "2023-07-09T11:58:51.892Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:59:01.355Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T11:59:54.012Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T12:00:55.775Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T12:01:21.346Z"
   },
   {
    "duration": 137,
    "start_time": "2023-07-09T12:03:11.719Z"
   },
   {
    "duration": 78,
    "start_time": "2023-07-09T12:04:09.618Z"
   },
   {
    "duration": 87,
    "start_time": "2023-07-09T12:05:35.446Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T12:09:46.590Z"
   },
   {
    "duration": 2117,
    "start_time": "2023-07-09T12:13:18.818Z"
   },
   {
    "duration": 148,
    "start_time": "2023-07-09T12:14:06.304Z"
   },
   {
    "duration": 4314,
    "start_time": "2023-07-09T12:14:19.121Z"
   },
   {
    "duration": 290,
    "start_time": "2023-07-09T12:14:41.177Z"
   },
   {
    "duration": 880,
    "start_time": "2023-07-09T12:15:09.845Z"
   },
   {
    "duration": 470,
    "start_time": "2023-07-09T12:16:18.273Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-09T12:17:00.414Z"
   },
   {
    "duration": 353,
    "start_time": "2023-07-09T12:17:05.134Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T12:17:08.308Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T12:39:03.815Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T12:39:47.839Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T12:40:20.430Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T12:40:46.289Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T12:41:10.918Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T12:41:36.119Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T12:41:44.231Z"
   },
   {
    "duration": 1464,
    "start_time": "2023-07-09T12:43:47.470Z"
   },
   {
    "duration": 2103,
    "start_time": "2023-07-09T12:43:48.936Z"
   },
   {
    "duration": 108,
    "start_time": "2023-07-09T12:43:51.041Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T12:43:51.151Z"
   },
   {
    "duration": 30880,
    "start_time": "2023-07-09T12:43:51.157Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T12:44:22.039Z"
   },
   {
    "duration": 304,
    "start_time": "2023-07-09T12:44:22.061Z"
   },
   {
    "duration": 123,
    "start_time": "2023-07-09T12:44:22.367Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.492Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.494Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.495Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.497Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.498Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.500Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.501Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.503Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.503Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.504Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.506Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.507Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.508Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:44:22.509Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T12:47:13.769Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-09T12:47:25.337Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T12:47:34.491Z"
   },
   {
    "duration": 1270,
    "start_time": "2023-07-09T12:50:52.888Z"
   },
   {
    "duration": 2148,
    "start_time": "2023-07-09T12:50:54.160Z"
   },
   {
    "duration": 55,
    "start_time": "2023-07-09T12:50:56.310Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T12:50:56.367Z"
   },
   {
    "duration": 14491,
    "start_time": "2023-07-09T12:50:56.374Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T12:51:10.866Z"
   },
   {
    "duration": 189,
    "start_time": "2023-07-09T12:51:10.888Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-09T12:51:11.079Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-09T12:51:11.097Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-09T12:51:11.109Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-09T12:51:11.121Z"
   },
   {
    "duration": 31,
    "start_time": "2023-07-09T12:51:11.139Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T12:51:11.171Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T12:51:11.183Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T12:51:11.190Z"
   },
   {
    "duration": 87,
    "start_time": "2023-07-09T12:51:11.199Z"
   },
   {
    "duration": 98,
    "start_time": "2023-07-09T12:51:11.288Z"
   },
   {
    "duration": 2157,
    "start_time": "2023-07-09T12:51:11.387Z"
   },
   {
    "duration": 2773,
    "start_time": "2023-07-09T12:51:13.547Z"
   },
   {
    "duration": 490,
    "start_time": "2023-07-09T12:51:16.322Z"
   },
   {
    "duration": 454,
    "start_time": "2023-07-09T12:51:16.814Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T12:51:17.270Z"
   },
   {
    "duration": 1382,
    "start_time": "2023-07-09T12:53:19.990Z"
   },
   {
    "duration": 2060,
    "start_time": "2023-07-09T12:53:21.374Z"
   },
   {
    "duration": 60,
    "start_time": "2023-07-09T12:53:23.436Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T12:53:23.498Z"
   },
   {
    "duration": 18524,
    "start_time": "2023-07-09T12:53:23.505Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T12:53:42.031Z"
   },
   {
    "duration": 325,
    "start_time": "2023-07-09T12:53:42.051Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.378Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.379Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.380Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.381Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.383Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.384Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.385Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.385Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.386Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.387Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.388Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.390Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.391Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.392Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T12:53:42.393Z"
   },
   {
    "duration": 115,
    "start_time": "2023-07-09T12:54:35.490Z"
   },
   {
    "duration": 68,
    "start_time": "2023-07-09T12:54:41.414Z"
   },
   {
    "duration": 88,
    "start_time": "2023-07-09T12:54:50.246Z"
   },
   {
    "duration": 1317,
    "start_time": "2023-07-09T12:54:55.673Z"
   },
   {
    "duration": 2105,
    "start_time": "2023-07-09T12:54:56.992Z"
   },
   {
    "duration": 71,
    "start_time": "2023-07-09T12:54:59.099Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T12:54:59.172Z"
   },
   {
    "duration": 17422,
    "start_time": "2023-07-09T12:54:59.178Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T12:55:16.602Z"
   },
   {
    "duration": 202,
    "start_time": "2023-07-09T12:55:16.623Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T12:55:16.827Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T12:55:16.848Z"
   },
   {
    "duration": 44,
    "start_time": "2023-07-09T12:55:16.864Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T12:55:16.910Z"
   },
   {
    "duration": 26,
    "start_time": "2023-07-09T12:55:16.920Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-09T12:55:16.947Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T12:55:16.963Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-09T12:55:16.969Z"
   },
   {
    "duration": 90,
    "start_time": "2023-07-09T12:55:16.982Z"
   },
   {
    "duration": 94,
    "start_time": "2023-07-09T12:55:17.073Z"
   },
   {
    "duration": 2379,
    "start_time": "2023-07-09T12:55:17.170Z"
   },
   {
    "duration": 3037,
    "start_time": "2023-07-09T12:55:19.551Z"
   },
   {
    "duration": 515,
    "start_time": "2023-07-09T12:55:22.589Z"
   },
   {
    "duration": 497,
    "start_time": "2023-07-09T12:55:23.106Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T12:55:23.605Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T13:42:21.484Z"
   },
   {
    "duration": 5182,
    "start_time": "2023-07-09T13:42:21.489Z"
   },
   {
    "duration": 470,
    "start_time": "2023-07-09T13:42:26.672Z"
   },
   {
    "duration": 248,
    "start_time": "2023-07-09T13:42:27.143Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:42:27.393Z"
   },
   {
    "duration": 16638,
    "start_time": "2023-07-09T13:42:27.400Z"
   },
   {
    "duration": 28,
    "start_time": "2023-07-09T13:42:44.040Z"
   },
   {
    "duration": 203,
    "start_time": "2023-07-09T13:42:44.069Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T13:42:44.274Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T13:42:44.295Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T13:42:44.304Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T13:42:44.313Z"
   },
   {
    "duration": 33,
    "start_time": "2023-07-09T13:42:44.321Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:42:44.356Z"
   },
   {
    "duration": 9106,
    "start_time": "2023-07-09T13:42:44.363Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:43:26.008Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:43:59.915Z"
   },
   {
    "duration": 4292,
    "start_time": "2023-07-09T13:43:59.921Z"
   },
   {
    "duration": 471,
    "start_time": "2023-07-09T13:44:04.215Z"
   },
   {
    "duration": 174,
    "start_time": "2023-07-09T13:44:04.688Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:44:04.864Z"
   },
   {
    "duration": 39776,
    "start_time": "2023-07-09T13:44:04.871Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T13:44:44.649Z"
   },
   {
    "duration": 291,
    "start_time": "2023-07-09T13:44:44.670Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T13:44:44.962Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-09T13:44:44.981Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-09T13:44:44.995Z"
   },
   {
    "duration": 58,
    "start_time": "2023-07-09T13:44:45.013Z"
   },
   {
    "duration": 22,
    "start_time": "2023-07-09T13:44:45.073Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-09T13:44:45.097Z"
   },
   {
    "duration": 57552,
    "start_time": "2023-07-09T13:44:45.114Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:45:42.668Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:46:34.048Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:47:05.311Z"
   },
   {
    "duration": 13843,
    "start_time": "2023-07-09T13:47:09.151Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:47:25.515Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T13:47:40.200Z"
   },
   {
    "duration": 7890,
    "start_time": "2023-07-09T13:47:41.728Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:47:51.020Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:48:15.260Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:48:58.499Z"
   },
   {
    "duration": 2843,
    "start_time": "2023-07-09T13:48:59.947Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:49:04.793Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:49:20.924Z"
   },
   {
    "duration": 639,
    "start_time": "2023-07-09T13:49:22.315Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:49:41.607Z"
   },
   {
    "duration": 663,
    "start_time": "2023-07-09T13:49:42.884Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:50:04.028Z"
   },
   {
    "duration": 773,
    "start_time": "2023-07-09T13:50:05.731Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:50:48.263Z"
   },
   {
    "duration": 5778,
    "start_time": "2023-07-09T13:50:48.270Z"
   },
   {
    "duration": 597,
    "start_time": "2023-07-09T13:50:54.050Z"
   },
   {
    "duration": 61,
    "start_time": "2023-07-09T13:50:54.649Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:50:54.713Z"
   },
   {
    "duration": 16740,
    "start_time": "2023-07-09T13:50:54.719Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T13:51:11.461Z"
   },
   {
    "duration": 228,
    "start_time": "2023-07-09T13:51:11.481Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T13:51:11.710Z"
   },
   {
    "duration": 35,
    "start_time": "2023-07-09T13:51:11.730Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T13:51:11.768Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-09T13:51:11.788Z"
   },
   {
    "duration": 40,
    "start_time": "2023-07-09T13:51:11.806Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-09T13:51:11.849Z"
   },
   {
    "duration": 99160,
    "start_time": "2023-07-09T13:51:11.866Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:52:51.028Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:53:16.595Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:53:35.539Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T13:54:34.555Z"
   },
   {
    "duration": 4311,
    "start_time": "2023-07-09T13:54:34.560Z"
   },
   {
    "duration": 503,
    "start_time": "2023-07-09T13:54:38.873Z"
   },
   {
    "duration": 66,
    "start_time": "2023-07-09T13:54:39.381Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T13:54:39.450Z"
   },
   {
    "duration": 14112,
    "start_time": "2023-07-09T13:54:39.456Z"
   },
   {
    "duration": 22,
    "start_time": "2023-07-09T13:54:53.569Z"
   },
   {
    "duration": 197,
    "start_time": "2023-07-09T13:54:53.592Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T13:54:53.790Z"
   },
   {
    "duration": 24,
    "start_time": "2023-07-09T13:54:53.813Z"
   },
   {
    "duration": 25,
    "start_time": "2023-07-09T13:54:53.838Z"
   },
   {
    "duration": 26,
    "start_time": "2023-07-09T13:54:53.864Z"
   },
   {
    "duration": 38,
    "start_time": "2023-07-09T13:54:53.892Z"
   },
   {
    "duration": 22,
    "start_time": "2023-07-09T13:54:53.931Z"
   },
   {
    "duration": 602,
    "start_time": "2023-07-09T13:54:53.955Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T13:54:54.559Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T14:01:30.487Z"
   },
   {
    "duration": 379,
    "start_time": "2023-07-09T14:01:32.195Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T14:01:40.518Z"
   },
   {
    "duration": 115451,
    "start_time": "2023-07-09T14:01:41.789Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:04:18.816Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T14:05:40.204Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:10:27.287Z"
   },
   {
    "duration": 13851,
    "start_time": "2023-07-09T14:10:27.296Z"
   },
   {
    "duration": 535,
    "start_time": "2023-07-09T14:10:41.150Z"
   },
   {
    "duration": 329,
    "start_time": "2023-07-09T14:10:41.689Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.020Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.022Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.023Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.024Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.025Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.026Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.027Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.028Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.029Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.030Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.032Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.033Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:10:42.052Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T14:11:24.315Z"
   },
   {
    "duration": 4374,
    "start_time": "2023-07-09T14:11:24.320Z"
   },
   {
    "duration": 506,
    "start_time": "2023-07-09T14:11:28.696Z"
   },
   {
    "duration": 457,
    "start_time": "2023-07-09T14:11:29.203Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T14:11:29.662Z"
   },
   {
    "duration": 18562,
    "start_time": "2023-07-09T14:11:29.669Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-09T14:11:48.233Z"
   },
   {
    "duration": 270,
    "start_time": "2023-07-09T14:11:48.258Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T14:11:48.530Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-09T14:11:48.552Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T14:11:48.569Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T14:11:48.589Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T14:11:48.600Z"
   },
   {
    "duration": 24,
    "start_time": "2023-07-09T14:11:48.622Z"
   },
   {
    "duration": 1474,
    "start_time": "2023-07-09T14:11:48.648Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T14:11:50.124Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:12:20.943Z"
   },
   {
    "duration": 1149,
    "start_time": "2023-07-09T14:12:22.193Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:12:39.593Z"
   },
   {
    "duration": 262351,
    "start_time": "2023-07-09T14:12:40.559Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T14:18:25.588Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:18:59.195Z"
   },
   {
    "duration": 4257,
    "start_time": "2023-07-09T14:19:49.634Z"
   },
   {
    "duration": 484,
    "start_time": "2023-07-09T14:19:53.893Z"
   },
   {
    "duration": 18328,
    "start_time": "2023-07-09T14:19:56.030Z"
   },
   {
    "duration": 21,
    "start_time": "2023-07-09T14:20:15.042Z"
   },
   {
    "duration": 203,
    "start_time": "2023-07-09T14:20:16.714Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T14:20:21.019Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T14:20:29.350Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T14:20:30.650Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T14:20:37.487Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:20:54.789Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:21:33.232Z"
   },
   {
    "duration": 351,
    "start_time": "2023-07-09T14:22:00.719Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T14:22:53.862Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T14:23:01.790Z"
   },
   {
    "duration": 22,
    "start_time": "2023-07-09T14:24:18.327Z"
   },
   {
    "duration": 32,
    "start_time": "2023-07-09T14:24:47.314Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T14:27:33.630Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T14:28:19.917Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:28:30.002Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T14:29:42.249Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T14:30:19.894Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-09T14:30:48.976Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T14:32:35.221Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-09T14:32:58.974Z"
   },
   {
    "duration": 131,
    "start_time": "2023-07-09T14:33:04.315Z"
   },
   {
    "duration": 125,
    "start_time": "2023-07-09T14:33:32.216Z"
   },
   {
    "duration": 133,
    "start_time": "2023-07-09T14:34:02.937Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T14:34:03.268Z"
   },
   {
    "duration": 42,
    "start_time": "2023-07-09T14:37:10.233Z"
   },
   {
    "duration": 39,
    "start_time": "2023-07-09T14:37:19.188Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T14:40:24.047Z"
   },
   {
    "duration": 4151,
    "start_time": "2023-07-09T14:40:24.057Z"
   },
   {
    "duration": 567,
    "start_time": "2023-07-09T14:40:28.211Z"
   },
   {
    "duration": 67,
    "start_time": "2023-07-09T14:40:28.779Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T14:40:28.848Z"
   },
   {
    "duration": 15165,
    "start_time": "2023-07-09T14:40:28.855Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T14:40:44.022Z"
   },
   {
    "duration": 226,
    "start_time": "2023-07-09T14:40:44.042Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T14:40:44.270Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T14:40:44.290Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T14:40:44.300Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-09T14:40:44.315Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T14:40:44.329Z"
   },
   {
    "duration": 217415,
    "start_time": "2023-07-09T14:40:44.336Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T14:44:21.754Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T14:44:21.761Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T14:59:36.271Z"
   },
   {
    "duration": 4093,
    "start_time": "2023-07-09T14:59:36.283Z"
   },
   {
    "duration": 517,
    "start_time": "2023-07-09T14:59:40.378Z"
   },
   {
    "duration": 203,
    "start_time": "2023-07-09T14:59:40.897Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T14:59:41.102Z"
   },
   {
    "duration": 36333,
    "start_time": "2023-07-09T14:59:41.108Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T15:00:17.443Z"
   },
   {
    "duration": 511,
    "start_time": "2023-07-09T15:00:17.464Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T15:00:17.977Z"
   },
   {
    "duration": 126,
    "start_time": "2023-07-09T15:00:17.998Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T15:00:18.126Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T15:00:18.137Z"
   },
   {
    "duration": 102,
    "start_time": "2023-07-09T15:00:18.147Z"
   },
   {
    "duration": 222205,
    "start_time": "2023-07-09T15:00:18.251Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T15:04:00.458Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T15:12:01.866Z"
   },
   {
    "duration": 5026,
    "start_time": "2023-07-09T15:12:01.872Z"
   },
   {
    "duration": 508,
    "start_time": "2023-07-09T15:12:06.901Z"
   },
   {
    "duration": 61,
    "start_time": "2023-07-09T15:12:07.411Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T15:12:07.474Z"
   },
   {
    "duration": 14519,
    "start_time": "2023-07-09T15:12:07.481Z"
   },
   {
    "duration": 21,
    "start_time": "2023-07-09T15:12:22.003Z"
   },
   {
    "duration": 334,
    "start_time": "2023-07-09T15:12:22.027Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T15:12:22.363Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-09T15:12:22.385Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T15:12:22.399Z"
   },
   {
    "duration": 25,
    "start_time": "2023-07-09T15:12:22.422Z"
   },
   {
    "duration": 63,
    "start_time": "2023-07-09T15:12:22.449Z"
   },
   {
    "duration": 2387,
    "start_time": "2023-07-09T15:12:22.515Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T15:12:24.905Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T15:31:03.320Z"
   },
   {
    "duration": 365,
    "start_time": "2023-07-09T15:31:04.750Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T15:33:15.476Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T15:33:54.543Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T15:33:54.544Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T15:33:55.849Z"
   },
   {
    "duration": 8368,
    "start_time": "2023-07-09T15:33:57.016Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T15:34:44.788Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T15:35:00.418Z"
   },
   {
    "duration": 4735,
    "start_time": "2023-07-09T15:35:00.425Z"
   },
   {
    "duration": 638,
    "start_time": "2023-07-09T15:35:05.163Z"
   },
   {
    "duration": 117,
    "start_time": "2023-07-09T15:35:05.803Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T15:35:05.922Z"
   },
   {
    "duration": 16037,
    "start_time": "2023-07-09T15:35:05.929Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T15:35:21.967Z"
   },
   {
    "duration": 189,
    "start_time": "2023-07-09T15:35:21.989Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-09T15:35:22.179Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-09T15:35:22.200Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-09T15:35:22.214Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-09T15:35:22.225Z"
   },
   {
    "duration": 15,
    "start_time": "2023-07-09T15:35:22.237Z"
   },
   {
    "duration": 343168,
    "start_time": "2023-07-09T15:35:22.254Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T15:41:05.425Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T15:45:46.223Z"
   },
   {
    "duration": 4645,
    "start_time": "2023-07-09T15:45:46.229Z"
   },
   {
    "duration": 548,
    "start_time": "2023-07-09T15:45:50.877Z"
   },
   {
    "duration": 71,
    "start_time": "2023-07-09T15:45:51.428Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T15:45:51.502Z"
   },
   {
    "duration": 22208,
    "start_time": "2023-07-09T15:45:51.510Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T15:46:13.720Z"
   },
   {
    "duration": 209,
    "start_time": "2023-07-09T15:46:13.741Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T15:46:13.952Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-09T15:46:13.971Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T15:46:13.984Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T15:46:13.992Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T15:46:14.002Z"
   },
   {
    "duration": 227541,
    "start_time": "2023-07-09T15:46:14.010Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T15:50:01.553Z"
   },
   {
    "duration": 1330,
    "start_time": "2023-07-09T15:51:24.067Z"
   },
   {
    "duration": 28,
    "start_time": "2023-07-09T15:51:38.586Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T15:51:59.195Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-09T15:52:05.994Z"
   },
   {
    "duration": 29,
    "start_time": "2023-07-09T15:55:29.218Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T15:56:05.393Z"
   },
   {
    "duration": 12,
    "start_time": "2023-07-09T15:56:12.174Z"
   },
   {
    "duration": 24,
    "start_time": "2023-07-09T15:56:44.831Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T15:57:35.822Z"
   },
   {
    "duration": 4631,
    "start_time": "2023-07-09T15:57:35.828Z"
   },
   {
    "duration": 529,
    "start_time": "2023-07-09T15:57:40.461Z"
   },
   {
    "duration": 67,
    "start_time": "2023-07-09T15:57:40.992Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T15:57:41.061Z"
   },
   {
    "duration": 14106,
    "start_time": "2023-07-09T15:57:41.068Z"
   },
   {
    "duration": 21,
    "start_time": "2023-07-09T15:57:55.176Z"
   },
   {
    "duration": 208,
    "start_time": "2023-07-09T15:57:55.199Z"
   },
   {
    "duration": 19,
    "start_time": "2023-07-09T15:57:55.409Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T15:57:55.430Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-09T15:57:55.438Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-09T15:57:55.449Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T15:57:55.460Z"
   },
   {
    "duration": 223731,
    "start_time": "2023-07-09T15:57:55.468Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T16:01:39.202Z"
   },
   {
    "duration": 360,
    "start_time": "2023-07-09T16:01:39.208Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-09T16:01:39.570Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T16:07:22.536Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T16:07:24.360Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T16:09:24.452Z"
   },
   {
    "duration": 291,
    "start_time": "2023-07-09T16:15:04.039Z"
   },
   {
    "duration": 534,
    "start_time": "2023-07-09T16:15:08.167Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T16:15:10.151Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-09T16:15:48.151Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T17:17:00.917Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T17:20:29.570Z"
   },
   {
    "duration": 3,
    "start_time": "2023-07-09T17:23:48.684Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T17:26:53.220Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-09T17:26:54.883Z"
   },
   {
    "duration": 13,
    "start_time": "2023-07-09T17:27:04.255Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T17:28:23.123Z"
   },
   {
    "duration": 676,
    "start_time": "2023-07-09T17:32:37.289Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-09T17:34:21.287Z"
   },
   {
    "duration": 470,
    "start_time": "2023-07-09T17:36:57.610Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-09T17:40:26.426Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T17:40:41.490Z"
   },
   {
    "duration": 25,
    "start_time": "2023-07-09T17:41:49.560Z"
   },
   {
    "duration": 390,
    "start_time": "2023-07-09T17:41:52.898Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-09T17:42:36.198Z"
   },
   {
    "duration": 24,
    "start_time": "2023-07-09T17:42:40.298Z"
   },
   {
    "duration": 457,
    "start_time": "2023-07-09T17:43:39.239Z"
   },
   {
    "duration": 34,
    "start_time": "2023-07-09T17:52:47.666Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-09T17:52:53.959Z"
   },
   {
    "duration": 39,
    "start_time": "2023-07-09T17:53:05.114Z"
   },
   {
    "duration": 29,
    "start_time": "2023-07-09T17:53:12.528Z"
   },
   {
    "duration": 786,
    "start_time": "2023-07-09T17:53:27.754Z"
   },
   {
    "duration": 1271,
    "start_time": "2023-07-09T17:53:50.678Z"
   },
   {
    "duration": 957,
    "start_time": "2023-07-09T17:54:06.066Z"
   },
   {
    "duration": 555,
    "start_time": "2023-07-09T17:55:21.489Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-09T17:55:45.429Z"
   },
   {
    "duration": 532,
    "start_time": "2023-07-09T17:55:53.693Z"
   },
   {
    "duration": 597,
    "start_time": "2023-07-10T06:44:38.516Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-10T06:44:47.875Z"
   },
   {
    "duration": 4737,
    "start_time": "2023-07-10T06:44:47.881Z"
   },
   {
    "duration": 570,
    "start_time": "2023-07-10T06:44:52.621Z"
   },
   {
    "duration": 145,
    "start_time": "2023-07-10T06:44:53.193Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-10T06:44:53.340Z"
   },
   {
    "duration": 87,
    "start_time": "2023-07-10T06:44:53.350Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-10T06:44:53.439Z"
   },
   {
    "duration": 37953,
    "start_time": "2023-07-10T06:44:53.447Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-10T06:45:31.402Z"
   },
   {
    "duration": 310,
    "start_time": "2023-07-10T06:45:31.427Z"
   },
   {
    "duration": 26,
    "start_time": "2023-07-10T06:45:31.740Z"
   },
   {
    "duration": 21,
    "start_time": "2023-07-10T06:45:31.768Z"
   },
   {
    "duration": 26,
    "start_time": "2023-07-10T06:45:31.791Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-10T06:45:31.819Z"
   },
   {
    "duration": 32,
    "start_time": "2023-07-10T06:45:31.835Z"
   },
   {
    "duration": 71101,
    "start_time": "2023-07-10T06:45:31.869Z"
   },
   {
    "duration": 28,
    "start_time": "2023-07-10T06:46:42.972Z"
   },
   {
    "duration": 14,
    "start_time": "2023-07-10T06:46:43.001Z"
   },
   {
    "duration": 21,
    "start_time": "2023-07-10T06:46:43.017Z"
   },
   {
    "duration": 28,
    "start_time": "2023-07-10T06:46:43.040Z"
   },
   {
    "duration": 10,
    "start_time": "2023-07-10T06:46:43.070Z"
   },
   {
    "duration": 20,
    "start_time": "2023-07-10T06:46:43.082Z"
   },
   {
    "duration": 404,
    "start_time": "2023-07-10T06:46:43.104Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T06:46:43.510Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T06:46:43.511Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T06:46:43.513Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T06:46:43.514Z"
   },
   {
    "duration": 0,
    "start_time": "2023-07-10T06:46:43.515Z"
   },
   {
    "duration": 445,
    "start_time": "2023-07-10T06:52:36.802Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-10T06:52:44.890Z"
   },
   {
    "duration": 629,
    "start_time": "2023-07-10T06:52:47.746Z"
   },
   {
    "duration": 694,
    "start_time": "2023-07-10T06:52:55.634Z"
   },
   {
    "duration": 578,
    "start_time": "2023-07-10T06:53:39.038Z"
   },
   {
    "duration": 33,
    "start_time": "2023-07-10T06:57:20.398Z"
   },
   {
    "duration": 622,
    "start_time": "2023-07-10T06:57:39.518Z"
   },
   {
    "duration": 638,
    "start_time": "2023-07-10T06:57:44.406Z"
   },
   {
    "duration": 828,
    "start_time": "2023-07-10T06:58:08.715Z"
   },
   {
    "duration": 787,
    "start_time": "2023-07-10T06:59:45.841Z"
   },
   {
    "duration": 755,
    "start_time": "2023-07-10T07:00:45.143Z"
   },
   {
    "duration": 863,
    "start_time": "2023-07-10T07:01:10.874Z"
   },
   {
    "duration": 31,
    "start_time": "2023-07-10T07:01:51.442Z"
   },
   {
    "duration": 813,
    "start_time": "2023-07-10T07:02:26.578Z"
   },
   {
    "duration": 812,
    "start_time": "2023-07-10T07:02:43.874Z"
   },
   {
    "duration": 765,
    "start_time": "2023-07-10T07:07:30.222Z"
   },
   {
    "duration": 25,
    "start_time": "2023-07-10T07:11:25.221Z"
   },
   {
    "duration": 79,
    "start_time": "2023-07-10T07:11:34.288Z"
   },
   {
    "duration": 569,
    "start_time": "2023-07-10T07:11:56.785Z"
   },
   {
    "duration": 630,
    "start_time": "2023-07-10T07:12:18.488Z"
   },
   {
    "duration": 664,
    "start_time": "2023-07-10T07:12:41.894Z"
   },
   {
    "duration": 588,
    "start_time": "2023-07-10T07:12:54.198Z"
   },
   {
    "duration": 639,
    "start_time": "2023-07-10T07:13:03.549Z"
   },
   {
    "duration": 27,
    "start_time": "2023-07-10T07:15:45.747Z"
   },
   {
    "duration": 26,
    "start_time": "2023-07-10T07:16:23.120Z"
   },
   {
    "duration": 25,
    "start_time": "2023-07-10T07:17:16.657Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-10T07:18:34.168Z"
   },
   {
    "duration": 7,
    "start_time": "2023-07-10T07:18:52.095Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-10T07:19:36.928Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-10T07:19:38.466Z"
   },
   {
    "duration": 912060,
    "start_time": "2023-07-10T07:19:44.322Z"
   },
   {
    "duration": 267,
    "start_time": "2023-07-10T07:38:16.663Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-10T07:38:25.047Z"
   },
   {
    "duration": 57,
    "start_time": "2023-07-10T07:38:27.528Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-10T07:39:04.523Z"
   },
   {
    "duration": 48,
    "start_time": "2023-07-10T07:39:07.122Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-10T07:39:54.963Z"
   },
   {
    "duration": 28875,
    "start_time": "2023-07-10T07:39:56.863Z"
   },
   {
    "duration": 8,
    "start_time": "2023-07-10T07:40:39.947Z"
   },
   {
    "duration": 2169835,
    "start_time": "2023-07-10T07:40:42.646Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-10T08:23:28.107Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-10T08:34:06.400Z"
   },
   {
    "duration": 4114,
    "start_time": "2023-07-10T08:34:27.413Z"
   },
   {
    "duration": 3623,
    "start_time": "2023-07-10T08:35:41.793Z"
   },
   {
    "duration": 3598,
    "start_time": "2023-07-10T08:39:37.380Z"
   },
   {
    "duration": 3557,
    "start_time": "2023-07-10T08:40:02.465Z"
   },
   {
    "duration": 92,
    "start_time": "2023-07-10T08:40:25.964Z"
   },
   {
    "duration": 28,
    "start_time": "2023-07-10T08:40:32.776Z"
   },
   {
    "duration": 34,
    "start_time": "2023-07-10T08:41:22.064Z"
   },
   {
    "duration": 3886,
    "start_time": "2023-07-10T08:41:37.731Z"
   },
   {
    "duration": 11,
    "start_time": "2023-07-10T08:41:55.115Z"
   },
   {
    "duration": 297,
    "start_time": "2023-07-10T09:32:42.679Z"
   },
   {
    "duration": 162,
    "start_time": "2023-07-10T09:57:14.619Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-10T10:19:55.830Z"
   },
   {
    "duration": 4356,
    "start_time": "2023-07-10T10:19:55.836Z"
   },
   {
    "duration": 572,
    "start_time": "2023-07-10T10:20:00.195Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-10T10:20:00.768Z"
   },
   {
    "duration": 195,
    "start_time": "2023-07-10T10:20:00.774Z"
   },
   {
    "duration": 35332,
    "start_time": "2023-07-10T10:20:00.972Z"
   },
   {
    "duration": 289,
    "start_time": "2023-07-10T10:20:36.306Z"
   },
   {
    "duration": 23,
    "start_time": "2023-07-10T10:20:36.597Z"
   },
   {
    "duration": 17,
    "start_time": "2023-07-10T10:20:36.622Z"
   },
   {
    "duration": 30,
    "start_time": "2023-07-10T10:20:36.641Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-10T10:20:36.673Z"
   },
   {
    "duration": 66313,
    "start_time": "2023-07-10T10:20:36.690Z"
   },
   {
    "duration": 18,
    "start_time": "2023-07-10T10:21:43.005Z"
   },
   {
    "duration": 27,
    "start_time": "2023-07-10T10:21:43.025Z"
   },
   {
    "duration": 423,
    "start_time": "2023-07-10T10:21:43.054Z"
   },
   {
    "duration": 5,
    "start_time": "2023-07-10T10:21:43.478Z"
   },
   {
    "duration": 1606149,
    "start_time": "2023-07-10T10:21:43.485Z"
   },
   {
    "duration": 4,
    "start_time": "2023-07-10T10:48:29.636Z"
   },
   {
    "duration": 3486,
    "start_time": "2023-07-10T10:48:29.642Z"
   },
   {
    "duration": 3231,
    "start_time": "2023-07-10T10:48:33.130Z"
   },
   {
    "duration": 141,
    "start_time": "2023-07-10T11:11:42.935Z"
   },
   {
    "duration": 118,
    "start_time": "2023-07-10T12:48:46.293Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215.625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
